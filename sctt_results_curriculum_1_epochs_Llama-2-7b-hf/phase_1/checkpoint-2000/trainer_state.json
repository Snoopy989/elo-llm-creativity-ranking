{
  "best_global_step": 1000,
  "best_metric": 1.1588571071624756,
  "best_model_checkpoint": "./sctt_results_curriculum_1_epochs_Llama-2-7b-hf/phase_1\\checkpoint-1000",
  "epoch": 0.572737686139748,
  "eval_steps": 1000,
  "global_step": 2000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0028636884306987398,
      "grad_norm": 79.57469940185547,
      "learning_rate": 4.5e-07,
      "loss": 0.9246,
      "step": 10
    },
    {
      "epoch": 0.0057273768613974796,
      "grad_norm": 127.5727767944336,
      "learning_rate": 9.5e-07,
      "loss": 0.8738,
      "step": 20
    },
    {
      "epoch": 0.00859106529209622,
      "grad_norm": 87.35374450683594,
      "learning_rate": 1.45e-06,
      "loss": 0.7182,
      "step": 30
    },
    {
      "epoch": 0.011454753722794959,
      "grad_norm": 155.85159301757812,
      "learning_rate": 1.95e-06,
      "loss": 1.7947,
      "step": 40
    },
    {
      "epoch": 0.0143184421534937,
      "grad_norm": 96.61840057373047,
      "learning_rate": 2.4500000000000003e-06,
      "loss": 1.699,
      "step": 50
    },
    {
      "epoch": 0.01718213058419244,
      "grad_norm": 51.01338195800781,
      "learning_rate": 2.95e-06,
      "loss": 1.823,
      "step": 60
    },
    {
      "epoch": 0.02004581901489118,
      "grad_norm": 86.18168640136719,
      "learning_rate": 3.4500000000000004e-06,
      "loss": 1.059,
      "step": 70
    },
    {
      "epoch": 0.022909507445589918,
      "grad_norm": 29.880569458007812,
      "learning_rate": 3.95e-06,
      "loss": 1.0621,
      "step": 80
    },
    {
      "epoch": 0.02577319587628866,
      "grad_norm": 142.8107452392578,
      "learning_rate": 4.45e-06,
      "loss": 1.2547,
      "step": 90
    },
    {
      "epoch": 0.0286368843069874,
      "grad_norm": 150.6685028076172,
      "learning_rate": 4.950000000000001e-06,
      "loss": 0.8234,
      "step": 100
    },
    {
      "epoch": 0.03150057273768614,
      "grad_norm": 79.3604736328125,
      "learning_rate": 5.45e-06,
      "loss": 1.335,
      "step": 110
    },
    {
      "epoch": 0.03436426116838488,
      "grad_norm": 57.21981430053711,
      "learning_rate": 5.95e-06,
      "loss": 0.9573,
      "step": 120
    },
    {
      "epoch": 0.03722794959908362,
      "grad_norm": 14.749129295349121,
      "learning_rate": 6.45e-06,
      "loss": 0.6009,
      "step": 130
    },
    {
      "epoch": 0.04009163802978236,
      "grad_norm": 144.7097930908203,
      "learning_rate": 6.950000000000001e-06,
      "loss": 2.0102,
      "step": 140
    },
    {
      "epoch": 0.0429553264604811,
      "grad_norm": 11.893170356750488,
      "learning_rate": 7.45e-06,
      "loss": 0.7426,
      "step": 150
    },
    {
      "epoch": 0.045819014891179836,
      "grad_norm": 26.94629669189453,
      "learning_rate": 7.95e-06,
      "loss": 0.5908,
      "step": 160
    },
    {
      "epoch": 0.04868270332187858,
      "grad_norm": 52.624820709228516,
      "learning_rate": 8.45e-06,
      "loss": 0.6422,
      "step": 170
    },
    {
      "epoch": 0.05154639175257732,
      "grad_norm": 33.033180236816406,
      "learning_rate": 8.95e-06,
      "loss": 1.1016,
      "step": 180
    },
    {
      "epoch": 0.05441008018327606,
      "grad_norm": 144.33352661132812,
      "learning_rate": 9.450000000000001e-06,
      "loss": 1.3828,
      "step": 190
    },
    {
      "epoch": 0.0572737686139748,
      "grad_norm": 157.7412109375,
      "learning_rate": 9.950000000000001e-06,
      "loss": 1.0101,
      "step": 200
    },
    {
      "epoch": 0.06013745704467354,
      "grad_norm": 177.59027099609375,
      "learning_rate": 1.045e-05,
      "loss": 2.3439,
      "step": 210
    },
    {
      "epoch": 0.06300114547537228,
      "grad_norm": 65.52720642089844,
      "learning_rate": 1.095e-05,
      "loss": 0.8731,
      "step": 220
    },
    {
      "epoch": 0.06586483390607102,
      "grad_norm": 169.60415649414062,
      "learning_rate": 1.145e-05,
      "loss": 1.5438,
      "step": 230
    },
    {
      "epoch": 0.06872852233676977,
      "grad_norm": 75.37397766113281,
      "learning_rate": 1.195e-05,
      "loss": 1.735,
      "step": 240
    },
    {
      "epoch": 0.0715922107674685,
      "grad_norm": 153.44161987304688,
      "learning_rate": 1.2450000000000001e-05,
      "loss": 2.4232,
      "step": 250
    },
    {
      "epoch": 0.07445589919816724,
      "grad_norm": 153.08489990234375,
      "learning_rate": 1.2950000000000001e-05,
      "loss": 1.427,
      "step": 260
    },
    {
      "epoch": 0.07731958762886598,
      "grad_norm": 23.26593780517578,
      "learning_rate": 1.3450000000000002e-05,
      "loss": 1.6374,
      "step": 270
    },
    {
      "epoch": 0.08018327605956473,
      "grad_norm": 172.59397888183594,
      "learning_rate": 1.3950000000000002e-05,
      "loss": 1.6996,
      "step": 280
    },
    {
      "epoch": 0.08304696449026346,
      "grad_norm": 39.52104568481445,
      "learning_rate": 1.4449999999999999e-05,
      "loss": 2.2068,
      "step": 290
    },
    {
      "epoch": 0.0859106529209622,
      "grad_norm": 19.315290451049805,
      "learning_rate": 1.4950000000000001e-05,
      "loss": 1.2206,
      "step": 300
    },
    {
      "epoch": 0.08877434135166094,
      "grad_norm": 137.10704040527344,
      "learning_rate": 1.545e-05,
      "loss": 0.8561,
      "step": 310
    },
    {
      "epoch": 0.09163802978235967,
      "grad_norm": 3.5110836029052734,
      "learning_rate": 1.595e-05,
      "loss": 2.4104,
      "step": 320
    },
    {
      "epoch": 0.09450171821305842,
      "grad_norm": 129.2257843017578,
      "learning_rate": 1.645e-05,
      "loss": 1.7295,
      "step": 330
    },
    {
      "epoch": 0.09736540664375716,
      "grad_norm": 28.052364349365234,
      "learning_rate": 1.6950000000000002e-05,
      "loss": 0.523,
      "step": 340
    },
    {
      "epoch": 0.1002290950744559,
      "grad_norm": 163.10592651367188,
      "learning_rate": 1.745e-05,
      "loss": 1.0118,
      "step": 350
    },
    {
      "epoch": 0.10309278350515463,
      "grad_norm": 124.40232849121094,
      "learning_rate": 1.795e-05,
      "loss": 1.9154,
      "step": 360
    },
    {
      "epoch": 0.10595647193585338,
      "grad_norm": 98.40947723388672,
      "learning_rate": 1.845e-05,
      "loss": 0.8547,
      "step": 370
    },
    {
      "epoch": 0.10882016036655212,
      "grad_norm": 28.401222229003906,
      "learning_rate": 1.895e-05,
      "loss": 1.0301,
      "step": 380
    },
    {
      "epoch": 0.11168384879725086,
      "grad_norm": 53.26777267456055,
      "learning_rate": 1.9450000000000002e-05,
      "loss": 1.9111,
      "step": 390
    },
    {
      "epoch": 0.1145475372279496,
      "grad_norm": 148.125,
      "learning_rate": 1.995e-05,
      "loss": 1.624,
      "step": 400
    },
    {
      "epoch": 0.11741122565864834,
      "grad_norm": 174.4932403564453,
      "learning_rate": 2.045e-05,
      "loss": 1.1434,
      "step": 410
    },
    {
      "epoch": 0.12027491408934708,
      "grad_norm": 178.27789306640625,
      "learning_rate": 2.095e-05,
      "loss": 2.0094,
      "step": 420
    },
    {
      "epoch": 0.12313860252004583,
      "grad_norm": 153.8032989501953,
      "learning_rate": 2.145e-05,
      "loss": 2.6538,
      "step": 430
    },
    {
      "epoch": 0.12600229095074456,
      "grad_norm": 162.99942016601562,
      "learning_rate": 2.195e-05,
      "loss": 1.3631,
      "step": 440
    },
    {
      "epoch": 0.12886597938144329,
      "grad_norm": 173.99794006347656,
      "learning_rate": 2.245e-05,
      "loss": 1.8303,
      "step": 450
    },
    {
      "epoch": 0.13172966781214204,
      "grad_norm": 2.1185061931610107,
      "learning_rate": 2.2950000000000002e-05,
      "loss": 1.5457,
      "step": 460
    },
    {
      "epoch": 0.13459335624284077,
      "grad_norm": 12.151549339294434,
      "learning_rate": 2.345e-05,
      "loss": 1.7921,
      "step": 470
    },
    {
      "epoch": 0.13745704467353953,
      "grad_norm": 123.99901580810547,
      "learning_rate": 2.395e-05,
      "loss": 1.7476,
      "step": 480
    },
    {
      "epoch": 0.14032073310423826,
      "grad_norm": 79.57270812988281,
      "learning_rate": 2.445e-05,
      "loss": 0.7064,
      "step": 490
    },
    {
      "epoch": 0.143184421534937,
      "grad_norm": 7.035595893859863,
      "learning_rate": 2.495e-05,
      "loss": 1.1987,
      "step": 500
    },
    {
      "epoch": 0.14604810996563575,
      "grad_norm": 90.12335205078125,
      "learning_rate": 2.5450000000000002e-05,
      "loss": 1.7331,
      "step": 510
    },
    {
      "epoch": 0.14891179839633448,
      "grad_norm": 10.40755558013916,
      "learning_rate": 2.595e-05,
      "loss": 1.3956,
      "step": 520
    },
    {
      "epoch": 0.1517754868270332,
      "grad_norm": 11.554871559143066,
      "learning_rate": 2.6450000000000003e-05,
      "loss": 1.6892,
      "step": 530
    },
    {
      "epoch": 0.15463917525773196,
      "grad_norm": 105.87930297851562,
      "learning_rate": 2.6950000000000005e-05,
      "loss": 1.1927,
      "step": 540
    },
    {
      "epoch": 0.1575028636884307,
      "grad_norm": 174.60321044921875,
      "learning_rate": 2.7450000000000003e-05,
      "loss": 2.2742,
      "step": 550
    },
    {
      "epoch": 0.16036655211912945,
      "grad_norm": 154.6477813720703,
      "learning_rate": 2.7950000000000005e-05,
      "loss": 1.5865,
      "step": 560
    },
    {
      "epoch": 0.16323024054982818,
      "grad_norm": 3.523946523666382,
      "learning_rate": 2.845e-05,
      "loss": 1.387,
      "step": 570
    },
    {
      "epoch": 0.1660939289805269,
      "grad_norm": 8.066291809082031,
      "learning_rate": 2.895e-05,
      "loss": 1.854,
      "step": 580
    },
    {
      "epoch": 0.16895761741122567,
      "grad_norm": 0.004578485619276762,
      "learning_rate": 2.945e-05,
      "loss": 0.7791,
      "step": 590
    },
    {
      "epoch": 0.1718213058419244,
      "grad_norm": 193.717041015625,
      "learning_rate": 2.995e-05,
      "loss": 2.8751,
      "step": 600
    },
    {
      "epoch": 0.17468499427262313,
      "grad_norm": 162.1082305908203,
      "learning_rate": 3.045e-05,
      "loss": 4.4869,
      "step": 610
    },
    {
      "epoch": 0.1775486827033219,
      "grad_norm": 10.160661697387695,
      "learning_rate": 3.095e-05,
      "loss": 1.9618,
      "step": 620
    },
    {
      "epoch": 0.18041237113402062,
      "grad_norm": 180.88133239746094,
      "learning_rate": 3.145e-05,
      "loss": 1.1641,
      "step": 630
    },
    {
      "epoch": 0.18327605956471935,
      "grad_norm": 4.278468132019043,
      "learning_rate": 3.1950000000000004e-05,
      "loss": 0.8125,
      "step": 640
    },
    {
      "epoch": 0.1861397479954181,
      "grad_norm": 111.17979431152344,
      "learning_rate": 3.245e-05,
      "loss": 0.9773,
      "step": 650
    },
    {
      "epoch": 0.18900343642611683,
      "grad_norm": 72.0273208618164,
      "learning_rate": 3.295e-05,
      "loss": 1.5189,
      "step": 660
    },
    {
      "epoch": 0.1918671248568156,
      "grad_norm": 22.57496452331543,
      "learning_rate": 3.345000000000001e-05,
      "loss": 0.4369,
      "step": 670
    },
    {
      "epoch": 0.19473081328751432,
      "grad_norm": 145.76510620117188,
      "learning_rate": 3.3950000000000005e-05,
      "loss": 1.9305,
      "step": 680
    },
    {
      "epoch": 0.19759450171821305,
      "grad_norm": 78.20345306396484,
      "learning_rate": 3.445e-05,
      "loss": 0.5265,
      "step": 690
    },
    {
      "epoch": 0.2004581901489118,
      "grad_norm": 177.4789276123047,
      "learning_rate": 3.495e-05,
      "loss": 1.9609,
      "step": 700
    },
    {
      "epoch": 0.20332187857961054,
      "grad_norm": 175.78622436523438,
      "learning_rate": 3.545e-05,
      "loss": 1.0614,
      "step": 710
    },
    {
      "epoch": 0.20618556701030927,
      "grad_norm": 181.46957397460938,
      "learning_rate": 3.595e-05,
      "loss": 0.9133,
      "step": 720
    },
    {
      "epoch": 0.20904925544100803,
      "grad_norm": 166.73020935058594,
      "learning_rate": 3.645e-05,
      "loss": 3.0033,
      "step": 730
    },
    {
      "epoch": 0.21191294387170675,
      "grad_norm": 177.95834350585938,
      "learning_rate": 3.6950000000000004e-05,
      "loss": 1.3657,
      "step": 740
    },
    {
      "epoch": 0.21477663230240548,
      "grad_norm": 178.87045288085938,
      "learning_rate": 3.745e-05,
      "loss": 2.3968,
      "step": 750
    },
    {
      "epoch": 0.21764032073310424,
      "grad_norm": 3.0540931224823,
      "learning_rate": 3.795e-05,
      "loss": 1.143,
      "step": 760
    },
    {
      "epoch": 0.22050400916380297,
      "grad_norm": 178.50057983398438,
      "learning_rate": 3.845e-05,
      "loss": 2.4636,
      "step": 770
    },
    {
      "epoch": 0.22336769759450173,
      "grad_norm": 1.2395052909851074,
      "learning_rate": 3.8950000000000005e-05,
      "loss": 2.9573,
      "step": 780
    },
    {
      "epoch": 0.22623138602520046,
      "grad_norm": 4.233154296875,
      "learning_rate": 3.9450000000000003e-05,
      "loss": 1.8664,
      "step": 790
    },
    {
      "epoch": 0.2290950744558992,
      "grad_norm": 6.921467304229736,
      "learning_rate": 3.995e-05,
      "loss": 1.5884,
      "step": 800
    },
    {
      "epoch": 0.23195876288659795,
      "grad_norm": 170.27525329589844,
      "learning_rate": 4.045000000000001e-05,
      "loss": 2.5943,
      "step": 810
    },
    {
      "epoch": 0.23482245131729668,
      "grad_norm": 118.09656524658203,
      "learning_rate": 4.095e-05,
      "loss": 1.3289,
      "step": 820
    },
    {
      "epoch": 0.2376861397479954,
      "grad_norm": 162.45057678222656,
      "learning_rate": 4.145e-05,
      "loss": 1.3327,
      "step": 830
    },
    {
      "epoch": 0.24054982817869416,
      "grad_norm": 87.85939025878906,
      "learning_rate": 4.195e-05,
      "loss": 1.495,
      "step": 840
    },
    {
      "epoch": 0.2434135166093929,
      "grad_norm": 180.3460235595703,
      "learning_rate": 4.245e-05,
      "loss": 1.1139,
      "step": 850
    },
    {
      "epoch": 0.24627720504009165,
      "grad_norm": 193.12057495117188,
      "learning_rate": 4.295e-05,
      "loss": 2.4603,
      "step": 860
    },
    {
      "epoch": 0.24914089347079038,
      "grad_norm": 139.45721435546875,
      "learning_rate": 4.345e-05,
      "loss": 1.5064,
      "step": 870
    },
    {
      "epoch": 0.2520045819014891,
      "grad_norm": 134.2423553466797,
      "learning_rate": 4.3950000000000004e-05,
      "loss": 1.8895,
      "step": 880
    },
    {
      "epoch": 0.25486827033218784,
      "grad_norm": 176.252197265625,
      "learning_rate": 4.445e-05,
      "loss": 2.7051,
      "step": 890
    },
    {
      "epoch": 0.25773195876288657,
      "grad_norm": 48.58606719970703,
      "learning_rate": 4.495e-05,
      "loss": 1.0121,
      "step": 900
    },
    {
      "epoch": 0.26059564719358536,
      "grad_norm": 7.177842140197754,
      "learning_rate": 4.545000000000001e-05,
      "loss": 0.9837,
      "step": 910
    },
    {
      "epoch": 0.2634593356242841,
      "grad_norm": 63.78435516357422,
      "learning_rate": 4.5950000000000006e-05,
      "loss": 1.2463,
      "step": 920
    },
    {
      "epoch": 0.2663230240549828,
      "grad_norm": 64.1390609741211,
      "learning_rate": 4.6450000000000004e-05,
      "loss": 0.861,
      "step": 930
    },
    {
      "epoch": 0.26918671248568155,
      "grad_norm": 69.67255401611328,
      "learning_rate": 4.695e-05,
      "loss": 1.2284,
      "step": 940
    },
    {
      "epoch": 0.2720504009163803,
      "grad_norm": 15.257335662841797,
      "learning_rate": 4.745e-05,
      "loss": 1.9696,
      "step": 950
    },
    {
      "epoch": 0.27491408934707906,
      "grad_norm": 1.3151649236679077,
      "learning_rate": 4.795e-05,
      "loss": 1.5772,
      "step": 960
    },
    {
      "epoch": 0.2777777777777778,
      "grad_norm": 0.1367829442024231,
      "learning_rate": 4.845e-05,
      "loss": 3.9821,
      "step": 970
    },
    {
      "epoch": 0.2806414662084765,
      "grad_norm": 0.7424548268318176,
      "learning_rate": 4.8950000000000004e-05,
      "loss": 3.8847,
      "step": 980
    },
    {
      "epoch": 0.28350515463917525,
      "grad_norm": 92.8092041015625,
      "learning_rate": 4.945e-05,
      "loss": 1.8531,
      "step": 990
    },
    {
      "epoch": 0.286368843069874,
      "grad_norm": 50.961883544921875,
      "learning_rate": 4.995e-05,
      "loss": 1.5218,
      "step": 1000
    },
    {
      "epoch": 0.286368843069874,
      "eval_loss": 1.1588571071624756,
      "eval_runtime": 12.3277,
      "eval_samples_per_second": 38.531,
      "eval_steps_per_second": 4.867,
      "step": 1000
    },
    {
      "epoch": 0.28923253150057276,
      "grad_norm": 120.02816772460938,
      "learning_rate": 4.9819422150882826e-05,
      "loss": 1.117,
      "step": 1010
    },
    {
      "epoch": 0.2920962199312715,
      "grad_norm": 96.89630126953125,
      "learning_rate": 4.961878009630819e-05,
      "loss": 1.1753,
      "step": 1020
    },
    {
      "epoch": 0.2949599083619702,
      "grad_norm": 38.81193923950195,
      "learning_rate": 4.941813804173355e-05,
      "loss": 0.3158,
      "step": 1030
    },
    {
      "epoch": 0.29782359679266895,
      "grad_norm": 192.26588439941406,
      "learning_rate": 4.921749598715891e-05,
      "loss": 2.6917,
      "step": 1040
    },
    {
      "epoch": 0.3006872852233677,
      "grad_norm": 186.8688507080078,
      "learning_rate": 4.901685393258427e-05,
      "loss": 2.4282,
      "step": 1050
    },
    {
      "epoch": 0.3035509736540664,
      "grad_norm": 124.64614868164062,
      "learning_rate": 4.881621187800963e-05,
      "loss": 2.7902,
      "step": 1060
    },
    {
      "epoch": 0.3064146620847652,
      "grad_norm": 129.28201293945312,
      "learning_rate": 4.861556982343499e-05,
      "loss": 1.0069,
      "step": 1070
    },
    {
      "epoch": 0.30927835051546393,
      "grad_norm": 35.087623596191406,
      "learning_rate": 4.8414927768860355e-05,
      "loss": 1.7756,
      "step": 1080
    },
    {
      "epoch": 0.31214203894616266,
      "grad_norm": 0.11355862766504288,
      "learning_rate": 4.8214285714285716e-05,
      "loss": 1.9659,
      "step": 1090
    },
    {
      "epoch": 0.3150057273768614,
      "grad_norm": 81.30125427246094,
      "learning_rate": 4.801364365971108e-05,
      "loss": 3.4853,
      "step": 1100
    },
    {
      "epoch": 0.3178694158075601,
      "grad_norm": 1.278221845626831,
      "learning_rate": 4.781300160513644e-05,
      "loss": 1.2942,
      "step": 1110
    },
    {
      "epoch": 0.3207331042382589,
      "grad_norm": 181.00440979003906,
      "learning_rate": 4.76123595505618e-05,
      "loss": 2.1617,
      "step": 1120
    },
    {
      "epoch": 0.32359679266895763,
      "grad_norm": 0.24414990842342377,
      "learning_rate": 4.741171749598716e-05,
      "loss": 0.62,
      "step": 1130
    },
    {
      "epoch": 0.32646048109965636,
      "grad_norm": 5.08853006362915,
      "learning_rate": 4.721107544141252e-05,
      "loss": 2.4257,
      "step": 1140
    },
    {
      "epoch": 0.3293241695303551,
      "grad_norm": 182.81771850585938,
      "learning_rate": 4.7010433386837884e-05,
      "loss": 1.1328,
      "step": 1150
    },
    {
      "epoch": 0.3321878579610538,
      "grad_norm": 0.09124334901571274,
      "learning_rate": 4.6809791332263245e-05,
      "loss": 0.2063,
      "step": 1160
    },
    {
      "epoch": 0.33505154639175255,
      "grad_norm": 182.80471801757812,
      "learning_rate": 4.6609149277688606e-05,
      "loss": 2.1463,
      "step": 1170
    },
    {
      "epoch": 0.33791523482245134,
      "grad_norm": 31.07621192932129,
      "learning_rate": 4.640850722311397e-05,
      "loss": 1.4904,
      "step": 1180
    },
    {
      "epoch": 0.34077892325315007,
      "grad_norm": 0.09062328189611435,
      "learning_rate": 4.620786516853933e-05,
      "loss": 1.5394,
      "step": 1190
    },
    {
      "epoch": 0.3436426116838488,
      "grad_norm": 204.43533325195312,
      "learning_rate": 4.600722311396469e-05,
      "loss": 1.5976,
      "step": 1200
    },
    {
      "epoch": 0.3465063001145475,
      "grad_norm": 143.2997589111328,
      "learning_rate": 4.580658105939005e-05,
      "loss": 2.3168,
      "step": 1210
    },
    {
      "epoch": 0.34936998854524626,
      "grad_norm": 0.003834401722997427,
      "learning_rate": 4.560593900481541e-05,
      "loss": 0.3245,
      "step": 1220
    },
    {
      "epoch": 0.35223367697594504,
      "grad_norm": 195.47422790527344,
      "learning_rate": 4.5405296950240774e-05,
      "loss": 2.2285,
      "step": 1230
    },
    {
      "epoch": 0.3550973654066438,
      "grad_norm": 18.089168548583984,
      "learning_rate": 4.5204654895666135e-05,
      "loss": 0.1946,
      "step": 1240
    },
    {
      "epoch": 0.3579610538373425,
      "grad_norm": 0.05046636238694191,
      "learning_rate": 4.5004012841091497e-05,
      "loss": 1.7403,
      "step": 1250
    },
    {
      "epoch": 0.36082474226804123,
      "grad_norm": 205.3721160888672,
      "learning_rate": 4.480337078651686e-05,
      "loss": 4.4688,
      "step": 1260
    },
    {
      "epoch": 0.36368843069873996,
      "grad_norm": 6.971169471740723,
      "learning_rate": 4.460272873194222e-05,
      "loss": 2.3661,
      "step": 1270
    },
    {
      "epoch": 0.3665521191294387,
      "grad_norm": 16.317832946777344,
      "learning_rate": 4.4402086677367574e-05,
      "loss": 1.0806,
      "step": 1280
    },
    {
      "epoch": 0.3694158075601375,
      "grad_norm": 0.25466498732566833,
      "learning_rate": 4.4201444622792935e-05,
      "loss": 2.4154,
      "step": 1290
    },
    {
      "epoch": 0.3722794959908362,
      "grad_norm": 0.48858949542045593,
      "learning_rate": 4.40008025682183e-05,
      "loss": 0.9755,
      "step": 1300
    },
    {
      "epoch": 0.37514318442153494,
      "grad_norm": 0.0010952969314530492,
      "learning_rate": 4.3800160513643664e-05,
      "loss": 2.4418,
      "step": 1310
    },
    {
      "epoch": 0.37800687285223367,
      "grad_norm": 227.02731323242188,
      "learning_rate": 4.359951845906902e-05,
      "loss": 2.1324,
      "step": 1320
    },
    {
      "epoch": 0.3808705612829324,
      "grad_norm": 0.0030246733222156763,
      "learning_rate": 4.339887640449438e-05,
      "loss": 4.3382,
      "step": 1330
    },
    {
      "epoch": 0.3837342497136312,
      "grad_norm": 7.684823989868164,
      "learning_rate": 4.319823434991975e-05,
      "loss": 0.2705,
      "step": 1340
    },
    {
      "epoch": 0.3865979381443299,
      "grad_norm": 0.2027161568403244,
      "learning_rate": 4.299759229534511e-05,
      "loss": 2.4871,
      "step": 1350
    },
    {
      "epoch": 0.38946162657502864,
      "grad_norm": 0.15788483619689941,
      "learning_rate": 4.2796950240770464e-05,
      "loss": 1.9898,
      "step": 1360
    },
    {
      "epoch": 0.39232531500572737,
      "grad_norm": 1.3668357133865356,
      "learning_rate": 4.2596308186195825e-05,
      "loss": 2.0426,
      "step": 1370
    },
    {
      "epoch": 0.3951890034364261,
      "grad_norm": 184.54440307617188,
      "learning_rate": 4.239566613162119e-05,
      "loss": 1.6754,
      "step": 1380
    },
    {
      "epoch": 0.39805269186712483,
      "grad_norm": 181.51486206054688,
      "learning_rate": 4.2195024077046555e-05,
      "loss": 1.9203,
      "step": 1390
    },
    {
      "epoch": 0.4009163802978236,
      "grad_norm": 160.9042205810547,
      "learning_rate": 4.199438202247191e-05,
      "loss": 2.5169,
      "step": 1400
    },
    {
      "epoch": 0.40378006872852235,
      "grad_norm": 2.936898946762085,
      "learning_rate": 4.179373996789727e-05,
      "loss": 1.5626,
      "step": 1410
    },
    {
      "epoch": 0.4066437571592211,
      "grad_norm": 10.316537857055664,
      "learning_rate": 4.159309791332263e-05,
      "loss": 2.3776,
      "step": 1420
    },
    {
      "epoch": 0.4095074455899198,
      "grad_norm": 0.9931005835533142,
      "learning_rate": 4.1392455858748e-05,
      "loss": 2.3696,
      "step": 1430
    },
    {
      "epoch": 0.41237113402061853,
      "grad_norm": 185.85606384277344,
      "learning_rate": 4.1191813804173354e-05,
      "loss": 2.5662,
      "step": 1440
    },
    {
      "epoch": 0.4152348224513173,
      "grad_norm": 158.4371337890625,
      "learning_rate": 4.0991171749598716e-05,
      "loss": 1.9402,
      "step": 1450
    },
    {
      "epoch": 0.41809851088201605,
      "grad_norm": 2.01560640335083,
      "learning_rate": 4.079052969502408e-05,
      "loss": 2.1793,
      "step": 1460
    },
    {
      "epoch": 0.4209621993127148,
      "grad_norm": 201.9884796142578,
      "learning_rate": 4.0589887640449445e-05,
      "loss": 3.1926,
      "step": 1470
    },
    {
      "epoch": 0.4238258877434135,
      "grad_norm": 134.90249633789062,
      "learning_rate": 4.03892455858748e-05,
      "loss": 1.3546,
      "step": 1480
    },
    {
      "epoch": 0.42668957617411224,
      "grad_norm": 199.17747497558594,
      "learning_rate": 4.018860353130016e-05,
      "loss": 0.9329,
      "step": 1490
    },
    {
      "epoch": 0.42955326460481097,
      "grad_norm": 1.3329417705535889,
      "learning_rate": 3.998796147672552e-05,
      "loss": 1.0723,
      "step": 1500
    },
    {
      "epoch": 0.43241695303550975,
      "grad_norm": 1.617841362953186,
      "learning_rate": 3.978731942215089e-05,
      "loss": 1.1924,
      "step": 1510
    },
    {
      "epoch": 0.4352806414662085,
      "grad_norm": 2.9058754444122314,
      "learning_rate": 3.9586677367576245e-05,
      "loss": 0.429,
      "step": 1520
    },
    {
      "epoch": 0.4381443298969072,
      "grad_norm": 196.6562042236328,
      "learning_rate": 3.9386035313001606e-05,
      "loss": 2.9021,
      "step": 1530
    },
    {
      "epoch": 0.44100801832760594,
      "grad_norm": 24.74730110168457,
      "learning_rate": 3.918539325842697e-05,
      "loss": 1.4252,
      "step": 1540
    },
    {
      "epoch": 0.4438717067583047,
      "grad_norm": 23.663650512695312,
      "learning_rate": 3.898475120385233e-05,
      "loss": 1.905,
      "step": 1550
    },
    {
      "epoch": 0.44673539518900346,
      "grad_norm": 13.525103569030762,
      "learning_rate": 3.878410914927769e-05,
      "loss": 1.6738,
      "step": 1560
    },
    {
      "epoch": 0.4495990836197022,
      "grad_norm": 0.1395205855369568,
      "learning_rate": 3.858346709470305e-05,
      "loss": 1.2336,
      "step": 1570
    },
    {
      "epoch": 0.4524627720504009,
      "grad_norm": 2.613743543624878,
      "learning_rate": 3.838282504012841e-05,
      "loss": 1.3101,
      "step": 1580
    },
    {
      "epoch": 0.45532646048109965,
      "grad_norm": 0.1494046449661255,
      "learning_rate": 3.8182182985553774e-05,
      "loss": 2.5813,
      "step": 1590
    },
    {
      "epoch": 0.4581901489117984,
      "grad_norm": 22.276649475097656,
      "learning_rate": 3.7981540930979135e-05,
      "loss": 1.9876,
      "step": 1600
    },
    {
      "epoch": 0.46105383734249716,
      "grad_norm": 223.91888427734375,
      "learning_rate": 3.7780898876404496e-05,
      "loss": 2.5092,
      "step": 1610
    },
    {
      "epoch": 0.4639175257731959,
      "grad_norm": 184.61953735351562,
      "learning_rate": 3.758025682182986e-05,
      "loss": 2.623,
      "step": 1620
    },
    {
      "epoch": 0.4667812142038946,
      "grad_norm": 7.365856647491455,
      "learning_rate": 3.737961476725522e-05,
      "loss": 0.7227,
      "step": 1630
    },
    {
      "epoch": 0.46964490263459335,
      "grad_norm": 1.6340620517730713,
      "learning_rate": 3.717897271268058e-05,
      "loss": 1.2495,
      "step": 1640
    },
    {
      "epoch": 0.4725085910652921,
      "grad_norm": 202.6216583251953,
      "learning_rate": 3.697833065810594e-05,
      "loss": 1.911,
      "step": 1650
    },
    {
      "epoch": 0.4753722794959908,
      "grad_norm": 7.842195510864258,
      "learning_rate": 3.67776886035313e-05,
      "loss": 0.874,
      "step": 1660
    },
    {
      "epoch": 0.4782359679266896,
      "grad_norm": 107.57694244384766,
      "learning_rate": 3.6577046548956664e-05,
      "loss": 1.5127,
      "step": 1670
    },
    {
      "epoch": 0.48109965635738833,
      "grad_norm": 0.0011041497346013784,
      "learning_rate": 3.637640449438202e-05,
      "loss": 4.1291,
      "step": 1680
    },
    {
      "epoch": 0.48396334478808706,
      "grad_norm": 16.11095428466797,
      "learning_rate": 3.617576243980739e-05,
      "loss": 2.5491,
      "step": 1690
    },
    {
      "epoch": 0.4868270332187858,
      "grad_norm": 172.0516357421875,
      "learning_rate": 3.597512038523275e-05,
      "loss": 3.4308,
      "step": 1700
    },
    {
      "epoch": 0.4896907216494845,
      "grad_norm": 158.36981201171875,
      "learning_rate": 3.577447833065811e-05,
      "loss": 1.506,
      "step": 1710
    },
    {
      "epoch": 0.4925544100801833,
      "grad_norm": 182.44802856445312,
      "learning_rate": 3.5573836276083464e-05,
      "loss": 0.9911,
      "step": 1720
    },
    {
      "epoch": 0.49541809851088203,
      "grad_norm": 0.02560213766992092,
      "learning_rate": 3.537319422150883e-05,
      "loss": 2.6304,
      "step": 1730
    },
    {
      "epoch": 0.49828178694158076,
      "grad_norm": 4.075151443481445,
      "learning_rate": 3.517255216693419e-05,
      "loss": 2.0144,
      "step": 1740
    },
    {
      "epoch": 0.5011454753722795,
      "grad_norm": 0.640552282333374,
      "learning_rate": 3.4971910112359554e-05,
      "loss": 1.1177,
      "step": 1750
    },
    {
      "epoch": 0.5040091638029782,
      "grad_norm": 16.243408203125,
      "learning_rate": 3.477126805778491e-05,
      "loss": 0.8799,
      "step": 1760
    },
    {
      "epoch": 0.506872852233677,
      "grad_norm": 190.56419372558594,
      "learning_rate": 3.457062600321028e-05,
      "loss": 2.0681,
      "step": 1770
    },
    {
      "epoch": 0.5097365406643757,
      "grad_norm": 0.1740126758813858,
      "learning_rate": 3.436998394863564e-05,
      "loss": 2.7885,
      "step": 1780
    },
    {
      "epoch": 0.5126002290950744,
      "grad_norm": 21.16952896118164,
      "learning_rate": 3.4169341894061e-05,
      "loss": 2.4021,
      "step": 1790
    },
    {
      "epoch": 0.5154639175257731,
      "grad_norm": 123.10442352294922,
      "learning_rate": 3.3968699839486354e-05,
      "loss": 2.0111,
      "step": 1800
    },
    {
      "epoch": 0.518327605956472,
      "grad_norm": 145.0422821044922,
      "learning_rate": 3.3768057784911715e-05,
      "loss": 2.6389,
      "step": 1810
    },
    {
      "epoch": 0.5211912943871707,
      "grad_norm": 0.31517064571380615,
      "learning_rate": 3.3567415730337083e-05,
      "loss": 2.7747,
      "step": 1820
    },
    {
      "epoch": 0.5240549828178694,
      "grad_norm": 57.192161560058594,
      "learning_rate": 3.3366773675762445e-05,
      "loss": 3.5823,
      "step": 1830
    },
    {
      "epoch": 0.5269186712485682,
      "grad_norm": 0.5419042706489563,
      "learning_rate": 3.31661316211878e-05,
      "loss": 1.5205,
      "step": 1840
    },
    {
      "epoch": 0.5297823596792669,
      "grad_norm": 1.2237638235092163,
      "learning_rate": 3.296548956661316e-05,
      "loss": 1.8874,
      "step": 1850
    },
    {
      "epoch": 0.5326460481099656,
      "grad_norm": 0.5576372146606445,
      "learning_rate": 3.276484751203853e-05,
      "loss": 0.8405,
      "step": 1860
    },
    {
      "epoch": 0.5355097365406644,
      "grad_norm": 0.19736140966415405,
      "learning_rate": 3.256420545746389e-05,
      "loss": 1.4293,
      "step": 1870
    },
    {
      "epoch": 0.5383734249713631,
      "grad_norm": 185.93276977539062,
      "learning_rate": 3.2363563402889244e-05,
      "loss": 3.3713,
      "step": 1880
    },
    {
      "epoch": 0.5412371134020618,
      "grad_norm": 0.1077083945274353,
      "learning_rate": 3.2162921348314606e-05,
      "loss": 0.8371,
      "step": 1890
    },
    {
      "epoch": 0.5441008018327605,
      "grad_norm": 205.4419708251953,
      "learning_rate": 3.1962279293739974e-05,
      "loss": 2.7169,
      "step": 1900
    },
    {
      "epoch": 0.5469644902634594,
      "grad_norm": 17.6505069732666,
      "learning_rate": 3.176163723916533e-05,
      "loss": 1.4348,
      "step": 1910
    },
    {
      "epoch": 0.5498281786941581,
      "grad_norm": 0.07896116375923157,
      "learning_rate": 3.156099518459069e-05,
      "loss": 0.6556,
      "step": 1920
    },
    {
      "epoch": 0.5526918671248569,
      "grad_norm": 186.40936279296875,
      "learning_rate": 3.136035313001605e-05,
      "loss": 3.2907,
      "step": 1930
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 205.45835876464844,
      "learning_rate": 3.115971107544142e-05,
      "loss": 0.9941,
      "step": 1940
    },
    {
      "epoch": 0.5584192439862543,
      "grad_norm": 76.02547454833984,
      "learning_rate": 3.0959069020866774e-05,
      "loss": 0.7566,
      "step": 1950
    },
    {
      "epoch": 0.561282932416953,
      "grad_norm": 48.3950080871582,
      "learning_rate": 3.0758426966292135e-05,
      "loss": 1.1413,
      "step": 1960
    },
    {
      "epoch": 0.5641466208476518,
      "grad_norm": 0.020451359450817108,
      "learning_rate": 3.0557784911717496e-05,
      "loss": 2.9302,
      "step": 1970
    },
    {
      "epoch": 0.5670103092783505,
      "grad_norm": 0.14367760717868805,
      "learning_rate": 3.0357142857142857e-05,
      "loss": 2.8495,
      "step": 1980
    },
    {
      "epoch": 0.5698739977090492,
      "grad_norm": 188.8407440185547,
      "learning_rate": 3.0156500802568222e-05,
      "loss": 3.4357,
      "step": 1990
    },
    {
      "epoch": 0.572737686139748,
      "grad_norm": 214.78857421875,
      "learning_rate": 2.995585874799358e-05,
      "loss": 2.8052,
      "step": 2000
    },
    {
      "epoch": 0.572737686139748,
      "eval_loss": 1.593748688697815,
      "eval_runtime": 12.4107,
      "eval_samples_per_second": 38.273,
      "eval_steps_per_second": 4.835,
      "step": 2000
    }
  ],
  "logging_steps": 10,
  "max_steps": 3492,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 1000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.2561384428642304e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
