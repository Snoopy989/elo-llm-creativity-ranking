{
  "best_global_step": 1000,
  "best_metric": 1.1588571071624756,
  "best_model_checkpoint": "./sctt_results_curriculum_1_epochs_Llama-2-7b-hf/phase_1\\checkpoint-1000",
  "epoch": 1.0,
  "eval_steps": 1000,
  "global_step": 3492,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0028636884306987398,
      "grad_norm": 79.57469940185547,
      "learning_rate": 4.5e-07,
      "loss": 0.9246,
      "step": 10
    },
    {
      "epoch": 0.0057273768613974796,
      "grad_norm": 127.5727767944336,
      "learning_rate": 9.5e-07,
      "loss": 0.8738,
      "step": 20
    },
    {
      "epoch": 0.00859106529209622,
      "grad_norm": 87.35374450683594,
      "learning_rate": 1.45e-06,
      "loss": 0.7182,
      "step": 30
    },
    {
      "epoch": 0.011454753722794959,
      "grad_norm": 155.85159301757812,
      "learning_rate": 1.95e-06,
      "loss": 1.7947,
      "step": 40
    },
    {
      "epoch": 0.0143184421534937,
      "grad_norm": 96.61840057373047,
      "learning_rate": 2.4500000000000003e-06,
      "loss": 1.699,
      "step": 50
    },
    {
      "epoch": 0.01718213058419244,
      "grad_norm": 51.01338195800781,
      "learning_rate": 2.95e-06,
      "loss": 1.823,
      "step": 60
    },
    {
      "epoch": 0.02004581901489118,
      "grad_norm": 86.18168640136719,
      "learning_rate": 3.4500000000000004e-06,
      "loss": 1.059,
      "step": 70
    },
    {
      "epoch": 0.022909507445589918,
      "grad_norm": 29.880569458007812,
      "learning_rate": 3.95e-06,
      "loss": 1.0621,
      "step": 80
    },
    {
      "epoch": 0.02577319587628866,
      "grad_norm": 142.8107452392578,
      "learning_rate": 4.45e-06,
      "loss": 1.2547,
      "step": 90
    },
    {
      "epoch": 0.0286368843069874,
      "grad_norm": 150.6685028076172,
      "learning_rate": 4.950000000000001e-06,
      "loss": 0.8234,
      "step": 100
    },
    {
      "epoch": 0.03150057273768614,
      "grad_norm": 79.3604736328125,
      "learning_rate": 5.45e-06,
      "loss": 1.335,
      "step": 110
    },
    {
      "epoch": 0.03436426116838488,
      "grad_norm": 57.21981430053711,
      "learning_rate": 5.95e-06,
      "loss": 0.9573,
      "step": 120
    },
    {
      "epoch": 0.03722794959908362,
      "grad_norm": 14.749129295349121,
      "learning_rate": 6.45e-06,
      "loss": 0.6009,
      "step": 130
    },
    {
      "epoch": 0.04009163802978236,
      "grad_norm": 144.7097930908203,
      "learning_rate": 6.950000000000001e-06,
      "loss": 2.0102,
      "step": 140
    },
    {
      "epoch": 0.0429553264604811,
      "grad_norm": 11.893170356750488,
      "learning_rate": 7.45e-06,
      "loss": 0.7426,
      "step": 150
    },
    {
      "epoch": 0.045819014891179836,
      "grad_norm": 26.94629669189453,
      "learning_rate": 7.95e-06,
      "loss": 0.5908,
      "step": 160
    },
    {
      "epoch": 0.04868270332187858,
      "grad_norm": 52.624820709228516,
      "learning_rate": 8.45e-06,
      "loss": 0.6422,
      "step": 170
    },
    {
      "epoch": 0.05154639175257732,
      "grad_norm": 33.033180236816406,
      "learning_rate": 8.95e-06,
      "loss": 1.1016,
      "step": 180
    },
    {
      "epoch": 0.05441008018327606,
      "grad_norm": 144.33352661132812,
      "learning_rate": 9.450000000000001e-06,
      "loss": 1.3828,
      "step": 190
    },
    {
      "epoch": 0.0572737686139748,
      "grad_norm": 157.7412109375,
      "learning_rate": 9.950000000000001e-06,
      "loss": 1.0101,
      "step": 200
    },
    {
      "epoch": 0.06013745704467354,
      "grad_norm": 177.59027099609375,
      "learning_rate": 1.045e-05,
      "loss": 2.3439,
      "step": 210
    },
    {
      "epoch": 0.06300114547537228,
      "grad_norm": 65.52720642089844,
      "learning_rate": 1.095e-05,
      "loss": 0.8731,
      "step": 220
    },
    {
      "epoch": 0.06586483390607102,
      "grad_norm": 169.60415649414062,
      "learning_rate": 1.145e-05,
      "loss": 1.5438,
      "step": 230
    },
    {
      "epoch": 0.06872852233676977,
      "grad_norm": 75.37397766113281,
      "learning_rate": 1.195e-05,
      "loss": 1.735,
      "step": 240
    },
    {
      "epoch": 0.0715922107674685,
      "grad_norm": 153.44161987304688,
      "learning_rate": 1.2450000000000001e-05,
      "loss": 2.4232,
      "step": 250
    },
    {
      "epoch": 0.07445589919816724,
      "grad_norm": 153.08489990234375,
      "learning_rate": 1.2950000000000001e-05,
      "loss": 1.427,
      "step": 260
    },
    {
      "epoch": 0.07731958762886598,
      "grad_norm": 23.26593780517578,
      "learning_rate": 1.3450000000000002e-05,
      "loss": 1.6374,
      "step": 270
    },
    {
      "epoch": 0.08018327605956473,
      "grad_norm": 172.59397888183594,
      "learning_rate": 1.3950000000000002e-05,
      "loss": 1.6996,
      "step": 280
    },
    {
      "epoch": 0.08304696449026346,
      "grad_norm": 39.52104568481445,
      "learning_rate": 1.4449999999999999e-05,
      "loss": 2.2068,
      "step": 290
    },
    {
      "epoch": 0.0859106529209622,
      "grad_norm": 19.315290451049805,
      "learning_rate": 1.4950000000000001e-05,
      "loss": 1.2206,
      "step": 300
    },
    {
      "epoch": 0.08877434135166094,
      "grad_norm": 137.10704040527344,
      "learning_rate": 1.545e-05,
      "loss": 0.8561,
      "step": 310
    },
    {
      "epoch": 0.09163802978235967,
      "grad_norm": 3.5110836029052734,
      "learning_rate": 1.595e-05,
      "loss": 2.4104,
      "step": 320
    },
    {
      "epoch": 0.09450171821305842,
      "grad_norm": 129.2257843017578,
      "learning_rate": 1.645e-05,
      "loss": 1.7295,
      "step": 330
    },
    {
      "epoch": 0.09736540664375716,
      "grad_norm": 28.052364349365234,
      "learning_rate": 1.6950000000000002e-05,
      "loss": 0.523,
      "step": 340
    },
    {
      "epoch": 0.1002290950744559,
      "grad_norm": 163.10592651367188,
      "learning_rate": 1.745e-05,
      "loss": 1.0118,
      "step": 350
    },
    {
      "epoch": 0.10309278350515463,
      "grad_norm": 124.40232849121094,
      "learning_rate": 1.795e-05,
      "loss": 1.9154,
      "step": 360
    },
    {
      "epoch": 0.10595647193585338,
      "grad_norm": 98.40947723388672,
      "learning_rate": 1.845e-05,
      "loss": 0.8547,
      "step": 370
    },
    {
      "epoch": 0.10882016036655212,
      "grad_norm": 28.401222229003906,
      "learning_rate": 1.895e-05,
      "loss": 1.0301,
      "step": 380
    },
    {
      "epoch": 0.11168384879725086,
      "grad_norm": 53.26777267456055,
      "learning_rate": 1.9450000000000002e-05,
      "loss": 1.9111,
      "step": 390
    },
    {
      "epoch": 0.1145475372279496,
      "grad_norm": 148.125,
      "learning_rate": 1.995e-05,
      "loss": 1.624,
      "step": 400
    },
    {
      "epoch": 0.11741122565864834,
      "grad_norm": 174.4932403564453,
      "learning_rate": 2.045e-05,
      "loss": 1.1434,
      "step": 410
    },
    {
      "epoch": 0.12027491408934708,
      "grad_norm": 178.27789306640625,
      "learning_rate": 2.095e-05,
      "loss": 2.0094,
      "step": 420
    },
    {
      "epoch": 0.12313860252004583,
      "grad_norm": 153.8032989501953,
      "learning_rate": 2.145e-05,
      "loss": 2.6538,
      "step": 430
    },
    {
      "epoch": 0.12600229095074456,
      "grad_norm": 162.99942016601562,
      "learning_rate": 2.195e-05,
      "loss": 1.3631,
      "step": 440
    },
    {
      "epoch": 0.12886597938144329,
      "grad_norm": 173.99794006347656,
      "learning_rate": 2.245e-05,
      "loss": 1.8303,
      "step": 450
    },
    {
      "epoch": 0.13172966781214204,
      "grad_norm": 2.1185061931610107,
      "learning_rate": 2.2950000000000002e-05,
      "loss": 1.5457,
      "step": 460
    },
    {
      "epoch": 0.13459335624284077,
      "grad_norm": 12.151549339294434,
      "learning_rate": 2.345e-05,
      "loss": 1.7921,
      "step": 470
    },
    {
      "epoch": 0.13745704467353953,
      "grad_norm": 123.99901580810547,
      "learning_rate": 2.395e-05,
      "loss": 1.7476,
      "step": 480
    },
    {
      "epoch": 0.14032073310423826,
      "grad_norm": 79.57270812988281,
      "learning_rate": 2.445e-05,
      "loss": 0.7064,
      "step": 490
    },
    {
      "epoch": 0.143184421534937,
      "grad_norm": 7.035595893859863,
      "learning_rate": 2.495e-05,
      "loss": 1.1987,
      "step": 500
    },
    {
      "epoch": 0.14604810996563575,
      "grad_norm": 90.12335205078125,
      "learning_rate": 2.5450000000000002e-05,
      "loss": 1.7331,
      "step": 510
    },
    {
      "epoch": 0.14891179839633448,
      "grad_norm": 10.40755558013916,
      "learning_rate": 2.595e-05,
      "loss": 1.3956,
      "step": 520
    },
    {
      "epoch": 0.1517754868270332,
      "grad_norm": 11.554871559143066,
      "learning_rate": 2.6450000000000003e-05,
      "loss": 1.6892,
      "step": 530
    },
    {
      "epoch": 0.15463917525773196,
      "grad_norm": 105.87930297851562,
      "learning_rate": 2.6950000000000005e-05,
      "loss": 1.1927,
      "step": 540
    },
    {
      "epoch": 0.1575028636884307,
      "grad_norm": 174.60321044921875,
      "learning_rate": 2.7450000000000003e-05,
      "loss": 2.2742,
      "step": 550
    },
    {
      "epoch": 0.16036655211912945,
      "grad_norm": 154.6477813720703,
      "learning_rate": 2.7950000000000005e-05,
      "loss": 1.5865,
      "step": 560
    },
    {
      "epoch": 0.16323024054982818,
      "grad_norm": 3.523946523666382,
      "learning_rate": 2.845e-05,
      "loss": 1.387,
      "step": 570
    },
    {
      "epoch": 0.1660939289805269,
      "grad_norm": 8.066291809082031,
      "learning_rate": 2.895e-05,
      "loss": 1.854,
      "step": 580
    },
    {
      "epoch": 0.16895761741122567,
      "grad_norm": 0.004578485619276762,
      "learning_rate": 2.945e-05,
      "loss": 0.7791,
      "step": 590
    },
    {
      "epoch": 0.1718213058419244,
      "grad_norm": 193.717041015625,
      "learning_rate": 2.995e-05,
      "loss": 2.8751,
      "step": 600
    },
    {
      "epoch": 0.17468499427262313,
      "grad_norm": 162.1082305908203,
      "learning_rate": 3.045e-05,
      "loss": 4.4869,
      "step": 610
    },
    {
      "epoch": 0.1775486827033219,
      "grad_norm": 10.160661697387695,
      "learning_rate": 3.095e-05,
      "loss": 1.9618,
      "step": 620
    },
    {
      "epoch": 0.18041237113402062,
      "grad_norm": 180.88133239746094,
      "learning_rate": 3.145e-05,
      "loss": 1.1641,
      "step": 630
    },
    {
      "epoch": 0.18327605956471935,
      "grad_norm": 4.278468132019043,
      "learning_rate": 3.1950000000000004e-05,
      "loss": 0.8125,
      "step": 640
    },
    {
      "epoch": 0.1861397479954181,
      "grad_norm": 111.17979431152344,
      "learning_rate": 3.245e-05,
      "loss": 0.9773,
      "step": 650
    },
    {
      "epoch": 0.18900343642611683,
      "grad_norm": 72.0273208618164,
      "learning_rate": 3.295e-05,
      "loss": 1.5189,
      "step": 660
    },
    {
      "epoch": 0.1918671248568156,
      "grad_norm": 22.57496452331543,
      "learning_rate": 3.345000000000001e-05,
      "loss": 0.4369,
      "step": 670
    },
    {
      "epoch": 0.19473081328751432,
      "grad_norm": 145.76510620117188,
      "learning_rate": 3.3950000000000005e-05,
      "loss": 1.9305,
      "step": 680
    },
    {
      "epoch": 0.19759450171821305,
      "grad_norm": 78.20345306396484,
      "learning_rate": 3.445e-05,
      "loss": 0.5265,
      "step": 690
    },
    {
      "epoch": 0.2004581901489118,
      "grad_norm": 177.4789276123047,
      "learning_rate": 3.495e-05,
      "loss": 1.9609,
      "step": 700
    },
    {
      "epoch": 0.20332187857961054,
      "grad_norm": 175.78622436523438,
      "learning_rate": 3.545e-05,
      "loss": 1.0614,
      "step": 710
    },
    {
      "epoch": 0.20618556701030927,
      "grad_norm": 181.46957397460938,
      "learning_rate": 3.595e-05,
      "loss": 0.9133,
      "step": 720
    },
    {
      "epoch": 0.20904925544100803,
      "grad_norm": 166.73020935058594,
      "learning_rate": 3.645e-05,
      "loss": 3.0033,
      "step": 730
    },
    {
      "epoch": 0.21191294387170675,
      "grad_norm": 177.95834350585938,
      "learning_rate": 3.6950000000000004e-05,
      "loss": 1.3657,
      "step": 740
    },
    {
      "epoch": 0.21477663230240548,
      "grad_norm": 178.87045288085938,
      "learning_rate": 3.745e-05,
      "loss": 2.3968,
      "step": 750
    },
    {
      "epoch": 0.21764032073310424,
      "grad_norm": 3.0540931224823,
      "learning_rate": 3.795e-05,
      "loss": 1.143,
      "step": 760
    },
    {
      "epoch": 0.22050400916380297,
      "grad_norm": 178.50057983398438,
      "learning_rate": 3.845e-05,
      "loss": 2.4636,
      "step": 770
    },
    {
      "epoch": 0.22336769759450173,
      "grad_norm": 1.2395052909851074,
      "learning_rate": 3.8950000000000005e-05,
      "loss": 2.9573,
      "step": 780
    },
    {
      "epoch": 0.22623138602520046,
      "grad_norm": 4.233154296875,
      "learning_rate": 3.9450000000000003e-05,
      "loss": 1.8664,
      "step": 790
    },
    {
      "epoch": 0.2290950744558992,
      "grad_norm": 6.921467304229736,
      "learning_rate": 3.995e-05,
      "loss": 1.5884,
      "step": 800
    },
    {
      "epoch": 0.23195876288659795,
      "grad_norm": 170.27525329589844,
      "learning_rate": 4.045000000000001e-05,
      "loss": 2.5943,
      "step": 810
    },
    {
      "epoch": 0.23482245131729668,
      "grad_norm": 118.09656524658203,
      "learning_rate": 4.095e-05,
      "loss": 1.3289,
      "step": 820
    },
    {
      "epoch": 0.2376861397479954,
      "grad_norm": 162.45057678222656,
      "learning_rate": 4.145e-05,
      "loss": 1.3327,
      "step": 830
    },
    {
      "epoch": 0.24054982817869416,
      "grad_norm": 87.85939025878906,
      "learning_rate": 4.195e-05,
      "loss": 1.495,
      "step": 840
    },
    {
      "epoch": 0.2434135166093929,
      "grad_norm": 180.3460235595703,
      "learning_rate": 4.245e-05,
      "loss": 1.1139,
      "step": 850
    },
    {
      "epoch": 0.24627720504009165,
      "grad_norm": 193.12057495117188,
      "learning_rate": 4.295e-05,
      "loss": 2.4603,
      "step": 860
    },
    {
      "epoch": 0.24914089347079038,
      "grad_norm": 139.45721435546875,
      "learning_rate": 4.345e-05,
      "loss": 1.5064,
      "step": 870
    },
    {
      "epoch": 0.2520045819014891,
      "grad_norm": 134.2423553466797,
      "learning_rate": 4.3950000000000004e-05,
      "loss": 1.8895,
      "step": 880
    },
    {
      "epoch": 0.25486827033218784,
      "grad_norm": 176.252197265625,
      "learning_rate": 4.445e-05,
      "loss": 2.7051,
      "step": 890
    },
    {
      "epoch": 0.25773195876288657,
      "grad_norm": 48.58606719970703,
      "learning_rate": 4.495e-05,
      "loss": 1.0121,
      "step": 900
    },
    {
      "epoch": 0.26059564719358536,
      "grad_norm": 7.177842140197754,
      "learning_rate": 4.545000000000001e-05,
      "loss": 0.9837,
      "step": 910
    },
    {
      "epoch": 0.2634593356242841,
      "grad_norm": 63.78435516357422,
      "learning_rate": 4.5950000000000006e-05,
      "loss": 1.2463,
      "step": 920
    },
    {
      "epoch": 0.2663230240549828,
      "grad_norm": 64.1390609741211,
      "learning_rate": 4.6450000000000004e-05,
      "loss": 0.861,
      "step": 930
    },
    {
      "epoch": 0.26918671248568155,
      "grad_norm": 69.67255401611328,
      "learning_rate": 4.695e-05,
      "loss": 1.2284,
      "step": 940
    },
    {
      "epoch": 0.2720504009163803,
      "grad_norm": 15.257335662841797,
      "learning_rate": 4.745e-05,
      "loss": 1.9696,
      "step": 950
    },
    {
      "epoch": 0.27491408934707906,
      "grad_norm": 1.3151649236679077,
      "learning_rate": 4.795e-05,
      "loss": 1.5772,
      "step": 960
    },
    {
      "epoch": 0.2777777777777778,
      "grad_norm": 0.1367829442024231,
      "learning_rate": 4.845e-05,
      "loss": 3.9821,
      "step": 970
    },
    {
      "epoch": 0.2806414662084765,
      "grad_norm": 0.7424548268318176,
      "learning_rate": 4.8950000000000004e-05,
      "loss": 3.8847,
      "step": 980
    },
    {
      "epoch": 0.28350515463917525,
      "grad_norm": 92.8092041015625,
      "learning_rate": 4.945e-05,
      "loss": 1.8531,
      "step": 990
    },
    {
      "epoch": 0.286368843069874,
      "grad_norm": 50.961883544921875,
      "learning_rate": 4.995e-05,
      "loss": 1.5218,
      "step": 1000
    },
    {
      "epoch": 0.286368843069874,
      "eval_loss": 1.1588571071624756,
      "eval_runtime": 12.3277,
      "eval_samples_per_second": 38.531,
      "eval_steps_per_second": 4.867,
      "step": 1000
    },
    {
      "epoch": 0.28923253150057276,
      "grad_norm": 120.02816772460938,
      "learning_rate": 4.9819422150882826e-05,
      "loss": 1.117,
      "step": 1010
    },
    {
      "epoch": 0.2920962199312715,
      "grad_norm": 96.89630126953125,
      "learning_rate": 4.961878009630819e-05,
      "loss": 1.1753,
      "step": 1020
    },
    {
      "epoch": 0.2949599083619702,
      "grad_norm": 38.81193923950195,
      "learning_rate": 4.941813804173355e-05,
      "loss": 0.3158,
      "step": 1030
    },
    {
      "epoch": 0.29782359679266895,
      "grad_norm": 192.26588439941406,
      "learning_rate": 4.921749598715891e-05,
      "loss": 2.6917,
      "step": 1040
    },
    {
      "epoch": 0.3006872852233677,
      "grad_norm": 186.8688507080078,
      "learning_rate": 4.901685393258427e-05,
      "loss": 2.4282,
      "step": 1050
    },
    {
      "epoch": 0.3035509736540664,
      "grad_norm": 124.64614868164062,
      "learning_rate": 4.881621187800963e-05,
      "loss": 2.7902,
      "step": 1060
    },
    {
      "epoch": 0.3064146620847652,
      "grad_norm": 129.28201293945312,
      "learning_rate": 4.861556982343499e-05,
      "loss": 1.0069,
      "step": 1070
    },
    {
      "epoch": 0.30927835051546393,
      "grad_norm": 35.087623596191406,
      "learning_rate": 4.8414927768860355e-05,
      "loss": 1.7756,
      "step": 1080
    },
    {
      "epoch": 0.31214203894616266,
      "grad_norm": 0.11355862766504288,
      "learning_rate": 4.8214285714285716e-05,
      "loss": 1.9659,
      "step": 1090
    },
    {
      "epoch": 0.3150057273768614,
      "grad_norm": 81.30125427246094,
      "learning_rate": 4.801364365971108e-05,
      "loss": 3.4853,
      "step": 1100
    },
    {
      "epoch": 0.3178694158075601,
      "grad_norm": 1.278221845626831,
      "learning_rate": 4.781300160513644e-05,
      "loss": 1.2942,
      "step": 1110
    },
    {
      "epoch": 0.3207331042382589,
      "grad_norm": 181.00440979003906,
      "learning_rate": 4.76123595505618e-05,
      "loss": 2.1617,
      "step": 1120
    },
    {
      "epoch": 0.32359679266895763,
      "grad_norm": 0.24414990842342377,
      "learning_rate": 4.741171749598716e-05,
      "loss": 0.62,
      "step": 1130
    },
    {
      "epoch": 0.32646048109965636,
      "grad_norm": 5.08853006362915,
      "learning_rate": 4.721107544141252e-05,
      "loss": 2.4257,
      "step": 1140
    },
    {
      "epoch": 0.3293241695303551,
      "grad_norm": 182.81771850585938,
      "learning_rate": 4.7010433386837884e-05,
      "loss": 1.1328,
      "step": 1150
    },
    {
      "epoch": 0.3321878579610538,
      "grad_norm": 0.09124334901571274,
      "learning_rate": 4.6809791332263245e-05,
      "loss": 0.2063,
      "step": 1160
    },
    {
      "epoch": 0.33505154639175255,
      "grad_norm": 182.80471801757812,
      "learning_rate": 4.6609149277688606e-05,
      "loss": 2.1463,
      "step": 1170
    },
    {
      "epoch": 0.33791523482245134,
      "grad_norm": 31.07621192932129,
      "learning_rate": 4.640850722311397e-05,
      "loss": 1.4904,
      "step": 1180
    },
    {
      "epoch": 0.34077892325315007,
      "grad_norm": 0.09062328189611435,
      "learning_rate": 4.620786516853933e-05,
      "loss": 1.5394,
      "step": 1190
    },
    {
      "epoch": 0.3436426116838488,
      "grad_norm": 204.43533325195312,
      "learning_rate": 4.600722311396469e-05,
      "loss": 1.5976,
      "step": 1200
    },
    {
      "epoch": 0.3465063001145475,
      "grad_norm": 143.2997589111328,
      "learning_rate": 4.580658105939005e-05,
      "loss": 2.3168,
      "step": 1210
    },
    {
      "epoch": 0.34936998854524626,
      "grad_norm": 0.003834401722997427,
      "learning_rate": 4.560593900481541e-05,
      "loss": 0.3245,
      "step": 1220
    },
    {
      "epoch": 0.35223367697594504,
      "grad_norm": 195.47422790527344,
      "learning_rate": 4.5405296950240774e-05,
      "loss": 2.2285,
      "step": 1230
    },
    {
      "epoch": 0.3550973654066438,
      "grad_norm": 18.089168548583984,
      "learning_rate": 4.5204654895666135e-05,
      "loss": 0.1946,
      "step": 1240
    },
    {
      "epoch": 0.3579610538373425,
      "grad_norm": 0.05046636238694191,
      "learning_rate": 4.5004012841091497e-05,
      "loss": 1.7403,
      "step": 1250
    },
    {
      "epoch": 0.36082474226804123,
      "grad_norm": 205.3721160888672,
      "learning_rate": 4.480337078651686e-05,
      "loss": 4.4688,
      "step": 1260
    },
    {
      "epoch": 0.36368843069873996,
      "grad_norm": 6.971169471740723,
      "learning_rate": 4.460272873194222e-05,
      "loss": 2.3661,
      "step": 1270
    },
    {
      "epoch": 0.3665521191294387,
      "grad_norm": 16.317832946777344,
      "learning_rate": 4.4402086677367574e-05,
      "loss": 1.0806,
      "step": 1280
    },
    {
      "epoch": 0.3694158075601375,
      "grad_norm": 0.25466498732566833,
      "learning_rate": 4.4201444622792935e-05,
      "loss": 2.4154,
      "step": 1290
    },
    {
      "epoch": 0.3722794959908362,
      "grad_norm": 0.48858949542045593,
      "learning_rate": 4.40008025682183e-05,
      "loss": 0.9755,
      "step": 1300
    },
    {
      "epoch": 0.37514318442153494,
      "grad_norm": 0.0010952969314530492,
      "learning_rate": 4.3800160513643664e-05,
      "loss": 2.4418,
      "step": 1310
    },
    {
      "epoch": 0.37800687285223367,
      "grad_norm": 227.02731323242188,
      "learning_rate": 4.359951845906902e-05,
      "loss": 2.1324,
      "step": 1320
    },
    {
      "epoch": 0.3808705612829324,
      "grad_norm": 0.0030246733222156763,
      "learning_rate": 4.339887640449438e-05,
      "loss": 4.3382,
      "step": 1330
    },
    {
      "epoch": 0.3837342497136312,
      "grad_norm": 7.684823989868164,
      "learning_rate": 4.319823434991975e-05,
      "loss": 0.2705,
      "step": 1340
    },
    {
      "epoch": 0.3865979381443299,
      "grad_norm": 0.2027161568403244,
      "learning_rate": 4.299759229534511e-05,
      "loss": 2.4871,
      "step": 1350
    },
    {
      "epoch": 0.38946162657502864,
      "grad_norm": 0.15788483619689941,
      "learning_rate": 4.2796950240770464e-05,
      "loss": 1.9898,
      "step": 1360
    },
    {
      "epoch": 0.39232531500572737,
      "grad_norm": 1.3668357133865356,
      "learning_rate": 4.2596308186195825e-05,
      "loss": 2.0426,
      "step": 1370
    },
    {
      "epoch": 0.3951890034364261,
      "grad_norm": 184.54440307617188,
      "learning_rate": 4.239566613162119e-05,
      "loss": 1.6754,
      "step": 1380
    },
    {
      "epoch": 0.39805269186712483,
      "grad_norm": 181.51486206054688,
      "learning_rate": 4.2195024077046555e-05,
      "loss": 1.9203,
      "step": 1390
    },
    {
      "epoch": 0.4009163802978236,
      "grad_norm": 160.9042205810547,
      "learning_rate": 4.199438202247191e-05,
      "loss": 2.5169,
      "step": 1400
    },
    {
      "epoch": 0.40378006872852235,
      "grad_norm": 2.936898946762085,
      "learning_rate": 4.179373996789727e-05,
      "loss": 1.5626,
      "step": 1410
    },
    {
      "epoch": 0.4066437571592211,
      "grad_norm": 10.316537857055664,
      "learning_rate": 4.159309791332263e-05,
      "loss": 2.3776,
      "step": 1420
    },
    {
      "epoch": 0.4095074455899198,
      "grad_norm": 0.9931005835533142,
      "learning_rate": 4.1392455858748e-05,
      "loss": 2.3696,
      "step": 1430
    },
    {
      "epoch": 0.41237113402061853,
      "grad_norm": 185.85606384277344,
      "learning_rate": 4.1191813804173354e-05,
      "loss": 2.5662,
      "step": 1440
    },
    {
      "epoch": 0.4152348224513173,
      "grad_norm": 158.4371337890625,
      "learning_rate": 4.0991171749598716e-05,
      "loss": 1.9402,
      "step": 1450
    },
    {
      "epoch": 0.41809851088201605,
      "grad_norm": 2.01560640335083,
      "learning_rate": 4.079052969502408e-05,
      "loss": 2.1793,
      "step": 1460
    },
    {
      "epoch": 0.4209621993127148,
      "grad_norm": 201.9884796142578,
      "learning_rate": 4.0589887640449445e-05,
      "loss": 3.1926,
      "step": 1470
    },
    {
      "epoch": 0.4238258877434135,
      "grad_norm": 134.90249633789062,
      "learning_rate": 4.03892455858748e-05,
      "loss": 1.3546,
      "step": 1480
    },
    {
      "epoch": 0.42668957617411224,
      "grad_norm": 199.17747497558594,
      "learning_rate": 4.018860353130016e-05,
      "loss": 0.9329,
      "step": 1490
    },
    {
      "epoch": 0.42955326460481097,
      "grad_norm": 1.3329417705535889,
      "learning_rate": 3.998796147672552e-05,
      "loss": 1.0723,
      "step": 1500
    },
    {
      "epoch": 0.43241695303550975,
      "grad_norm": 1.617841362953186,
      "learning_rate": 3.978731942215089e-05,
      "loss": 1.1924,
      "step": 1510
    },
    {
      "epoch": 0.4352806414662085,
      "grad_norm": 2.9058754444122314,
      "learning_rate": 3.9586677367576245e-05,
      "loss": 0.429,
      "step": 1520
    },
    {
      "epoch": 0.4381443298969072,
      "grad_norm": 196.6562042236328,
      "learning_rate": 3.9386035313001606e-05,
      "loss": 2.9021,
      "step": 1530
    },
    {
      "epoch": 0.44100801832760594,
      "grad_norm": 24.74730110168457,
      "learning_rate": 3.918539325842697e-05,
      "loss": 1.4252,
      "step": 1540
    },
    {
      "epoch": 0.4438717067583047,
      "grad_norm": 23.663650512695312,
      "learning_rate": 3.898475120385233e-05,
      "loss": 1.905,
      "step": 1550
    },
    {
      "epoch": 0.44673539518900346,
      "grad_norm": 13.525103569030762,
      "learning_rate": 3.878410914927769e-05,
      "loss": 1.6738,
      "step": 1560
    },
    {
      "epoch": 0.4495990836197022,
      "grad_norm": 0.1395205855369568,
      "learning_rate": 3.858346709470305e-05,
      "loss": 1.2336,
      "step": 1570
    },
    {
      "epoch": 0.4524627720504009,
      "grad_norm": 2.613743543624878,
      "learning_rate": 3.838282504012841e-05,
      "loss": 1.3101,
      "step": 1580
    },
    {
      "epoch": 0.45532646048109965,
      "grad_norm": 0.1494046449661255,
      "learning_rate": 3.8182182985553774e-05,
      "loss": 2.5813,
      "step": 1590
    },
    {
      "epoch": 0.4581901489117984,
      "grad_norm": 22.276649475097656,
      "learning_rate": 3.7981540930979135e-05,
      "loss": 1.9876,
      "step": 1600
    },
    {
      "epoch": 0.46105383734249716,
      "grad_norm": 223.91888427734375,
      "learning_rate": 3.7780898876404496e-05,
      "loss": 2.5092,
      "step": 1610
    },
    {
      "epoch": 0.4639175257731959,
      "grad_norm": 184.61953735351562,
      "learning_rate": 3.758025682182986e-05,
      "loss": 2.623,
      "step": 1620
    },
    {
      "epoch": 0.4667812142038946,
      "grad_norm": 7.365856647491455,
      "learning_rate": 3.737961476725522e-05,
      "loss": 0.7227,
      "step": 1630
    },
    {
      "epoch": 0.46964490263459335,
      "grad_norm": 1.6340620517730713,
      "learning_rate": 3.717897271268058e-05,
      "loss": 1.2495,
      "step": 1640
    },
    {
      "epoch": 0.4725085910652921,
      "grad_norm": 202.6216583251953,
      "learning_rate": 3.697833065810594e-05,
      "loss": 1.911,
      "step": 1650
    },
    {
      "epoch": 0.4753722794959908,
      "grad_norm": 7.842195510864258,
      "learning_rate": 3.67776886035313e-05,
      "loss": 0.874,
      "step": 1660
    },
    {
      "epoch": 0.4782359679266896,
      "grad_norm": 107.57694244384766,
      "learning_rate": 3.6577046548956664e-05,
      "loss": 1.5127,
      "step": 1670
    },
    {
      "epoch": 0.48109965635738833,
      "grad_norm": 0.0011041497346013784,
      "learning_rate": 3.637640449438202e-05,
      "loss": 4.1291,
      "step": 1680
    },
    {
      "epoch": 0.48396334478808706,
      "grad_norm": 16.11095428466797,
      "learning_rate": 3.617576243980739e-05,
      "loss": 2.5491,
      "step": 1690
    },
    {
      "epoch": 0.4868270332187858,
      "grad_norm": 172.0516357421875,
      "learning_rate": 3.597512038523275e-05,
      "loss": 3.4308,
      "step": 1700
    },
    {
      "epoch": 0.4896907216494845,
      "grad_norm": 158.36981201171875,
      "learning_rate": 3.577447833065811e-05,
      "loss": 1.506,
      "step": 1710
    },
    {
      "epoch": 0.4925544100801833,
      "grad_norm": 182.44802856445312,
      "learning_rate": 3.5573836276083464e-05,
      "loss": 0.9911,
      "step": 1720
    },
    {
      "epoch": 0.49541809851088203,
      "grad_norm": 0.02560213766992092,
      "learning_rate": 3.537319422150883e-05,
      "loss": 2.6304,
      "step": 1730
    },
    {
      "epoch": 0.49828178694158076,
      "grad_norm": 4.075151443481445,
      "learning_rate": 3.517255216693419e-05,
      "loss": 2.0144,
      "step": 1740
    },
    {
      "epoch": 0.5011454753722795,
      "grad_norm": 0.640552282333374,
      "learning_rate": 3.4971910112359554e-05,
      "loss": 1.1177,
      "step": 1750
    },
    {
      "epoch": 0.5040091638029782,
      "grad_norm": 16.243408203125,
      "learning_rate": 3.477126805778491e-05,
      "loss": 0.8799,
      "step": 1760
    },
    {
      "epoch": 0.506872852233677,
      "grad_norm": 190.56419372558594,
      "learning_rate": 3.457062600321028e-05,
      "loss": 2.0681,
      "step": 1770
    },
    {
      "epoch": 0.5097365406643757,
      "grad_norm": 0.1740126758813858,
      "learning_rate": 3.436998394863564e-05,
      "loss": 2.7885,
      "step": 1780
    },
    {
      "epoch": 0.5126002290950744,
      "grad_norm": 21.16952896118164,
      "learning_rate": 3.4169341894061e-05,
      "loss": 2.4021,
      "step": 1790
    },
    {
      "epoch": 0.5154639175257731,
      "grad_norm": 123.10442352294922,
      "learning_rate": 3.3968699839486354e-05,
      "loss": 2.0111,
      "step": 1800
    },
    {
      "epoch": 0.518327605956472,
      "grad_norm": 145.0422821044922,
      "learning_rate": 3.3768057784911715e-05,
      "loss": 2.6389,
      "step": 1810
    },
    {
      "epoch": 0.5211912943871707,
      "grad_norm": 0.31517064571380615,
      "learning_rate": 3.3567415730337083e-05,
      "loss": 2.7747,
      "step": 1820
    },
    {
      "epoch": 0.5240549828178694,
      "grad_norm": 57.192161560058594,
      "learning_rate": 3.3366773675762445e-05,
      "loss": 3.5823,
      "step": 1830
    },
    {
      "epoch": 0.5269186712485682,
      "grad_norm": 0.5419042706489563,
      "learning_rate": 3.31661316211878e-05,
      "loss": 1.5205,
      "step": 1840
    },
    {
      "epoch": 0.5297823596792669,
      "grad_norm": 1.2237638235092163,
      "learning_rate": 3.296548956661316e-05,
      "loss": 1.8874,
      "step": 1850
    },
    {
      "epoch": 0.5326460481099656,
      "grad_norm": 0.5576372146606445,
      "learning_rate": 3.276484751203853e-05,
      "loss": 0.8405,
      "step": 1860
    },
    {
      "epoch": 0.5355097365406644,
      "grad_norm": 0.19736140966415405,
      "learning_rate": 3.256420545746389e-05,
      "loss": 1.4293,
      "step": 1870
    },
    {
      "epoch": 0.5383734249713631,
      "grad_norm": 185.93276977539062,
      "learning_rate": 3.2363563402889244e-05,
      "loss": 3.3713,
      "step": 1880
    },
    {
      "epoch": 0.5412371134020618,
      "grad_norm": 0.1077083945274353,
      "learning_rate": 3.2162921348314606e-05,
      "loss": 0.8371,
      "step": 1890
    },
    {
      "epoch": 0.5441008018327605,
      "grad_norm": 205.4419708251953,
      "learning_rate": 3.1962279293739974e-05,
      "loss": 2.7169,
      "step": 1900
    },
    {
      "epoch": 0.5469644902634594,
      "grad_norm": 17.6505069732666,
      "learning_rate": 3.176163723916533e-05,
      "loss": 1.4348,
      "step": 1910
    },
    {
      "epoch": 0.5498281786941581,
      "grad_norm": 0.07896116375923157,
      "learning_rate": 3.156099518459069e-05,
      "loss": 0.6556,
      "step": 1920
    },
    {
      "epoch": 0.5526918671248569,
      "grad_norm": 186.40936279296875,
      "learning_rate": 3.136035313001605e-05,
      "loss": 3.2907,
      "step": 1930
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 205.45835876464844,
      "learning_rate": 3.115971107544142e-05,
      "loss": 0.9941,
      "step": 1940
    },
    {
      "epoch": 0.5584192439862543,
      "grad_norm": 76.02547454833984,
      "learning_rate": 3.0959069020866774e-05,
      "loss": 0.7566,
      "step": 1950
    },
    {
      "epoch": 0.561282932416953,
      "grad_norm": 48.3950080871582,
      "learning_rate": 3.0758426966292135e-05,
      "loss": 1.1413,
      "step": 1960
    },
    {
      "epoch": 0.5641466208476518,
      "grad_norm": 0.020451359450817108,
      "learning_rate": 3.0557784911717496e-05,
      "loss": 2.9302,
      "step": 1970
    },
    {
      "epoch": 0.5670103092783505,
      "grad_norm": 0.14367760717868805,
      "learning_rate": 3.0357142857142857e-05,
      "loss": 2.8495,
      "step": 1980
    },
    {
      "epoch": 0.5698739977090492,
      "grad_norm": 188.8407440185547,
      "learning_rate": 3.0156500802568222e-05,
      "loss": 3.4357,
      "step": 1990
    },
    {
      "epoch": 0.572737686139748,
      "grad_norm": 214.78857421875,
      "learning_rate": 2.995585874799358e-05,
      "loss": 2.8052,
      "step": 2000
    },
    {
      "epoch": 0.572737686139748,
      "eval_loss": 1.593748688697815,
      "eval_runtime": 12.4107,
      "eval_samples_per_second": 38.273,
      "eval_steps_per_second": 4.835,
      "step": 2000
    },
    {
      "epoch": 0.5756013745704467,
      "grad_norm": 203.31210327148438,
      "learning_rate": 2.975521669341894e-05,
      "loss": 2.4892,
      "step": 2010
    },
    {
      "epoch": 0.5784650630011455,
      "grad_norm": 1.2093991041183472,
      "learning_rate": 2.9554574638844303e-05,
      "loss": 1.2335,
      "step": 2020
    },
    {
      "epoch": 0.5813287514318443,
      "grad_norm": 5.3341169357299805,
      "learning_rate": 2.9353932584269667e-05,
      "loss": 0.9346,
      "step": 2030
    },
    {
      "epoch": 0.584192439862543,
      "grad_norm": 155.8668975830078,
      "learning_rate": 2.9153290529695025e-05,
      "loss": 1.8026,
      "step": 2040
    },
    {
      "epoch": 0.5870561282932417,
      "grad_norm": 196.08705139160156,
      "learning_rate": 2.8952648475120386e-05,
      "loss": 2.138,
      "step": 2050
    },
    {
      "epoch": 0.5899198167239404,
      "grad_norm": 224.2455291748047,
      "learning_rate": 2.8752006420545748e-05,
      "loss": 2.7561,
      "step": 2060
    },
    {
      "epoch": 0.5927835051546392,
      "grad_norm": 6.323849201202393,
      "learning_rate": 2.8551364365971112e-05,
      "loss": 0.6553,
      "step": 2070
    },
    {
      "epoch": 0.5956471935853379,
      "grad_norm": 34.41400909423828,
      "learning_rate": 2.835072231139647e-05,
      "loss": 1.046,
      "step": 2080
    },
    {
      "epoch": 0.5985108820160366,
      "grad_norm": 188.66720581054688,
      "learning_rate": 2.815008025682183e-05,
      "loss": 1.9458,
      "step": 2090
    },
    {
      "epoch": 0.6013745704467354,
      "grad_norm": 2.2679429054260254,
      "learning_rate": 2.7949438202247193e-05,
      "loss": 1.4782,
      "step": 2100
    },
    {
      "epoch": 0.6042382588774341,
      "grad_norm": 0.09871382266283035,
      "learning_rate": 2.774879614767255e-05,
      "loss": 1.147,
      "step": 2110
    },
    {
      "epoch": 0.6071019473081328,
      "grad_norm": 0.445799857378006,
      "learning_rate": 2.7548154093097915e-05,
      "loss": 2.4747,
      "step": 2120
    },
    {
      "epoch": 0.6099656357388317,
      "grad_norm": 219.97286987304688,
      "learning_rate": 2.7347512038523277e-05,
      "loss": 1.0543,
      "step": 2130
    },
    {
      "epoch": 0.6128293241695304,
      "grad_norm": 0.1439618170261383,
      "learning_rate": 2.7146869983948635e-05,
      "loss": 2.0997,
      "step": 2140
    },
    {
      "epoch": 0.6156930126002291,
      "grad_norm": 0.060608286410570145,
      "learning_rate": 2.6946227929373996e-05,
      "loss": 2.9268,
      "step": 2150
    },
    {
      "epoch": 0.6185567010309279,
      "grad_norm": 8.081942558288574,
      "learning_rate": 2.674558587479936e-05,
      "loss": 2.4404,
      "step": 2160
    },
    {
      "epoch": 0.6214203894616266,
      "grad_norm": 0.22014056146144867,
      "learning_rate": 2.6544943820224722e-05,
      "loss": 0.8997,
      "step": 2170
    },
    {
      "epoch": 0.6242840778923253,
      "grad_norm": 192.24774169921875,
      "learning_rate": 2.634430176565008e-05,
      "loss": 2.1503,
      "step": 2180
    },
    {
      "epoch": 0.627147766323024,
      "grad_norm": 188.65377807617188,
      "learning_rate": 2.614365971107544e-05,
      "loss": 1.821,
      "step": 2190
    },
    {
      "epoch": 0.6300114547537228,
      "grad_norm": 0.2731793224811554,
      "learning_rate": 2.5943017656500806e-05,
      "loss": 1.2927,
      "step": 2200
    },
    {
      "epoch": 0.6328751431844215,
      "grad_norm": 1.0302150249481201,
      "learning_rate": 2.5742375601926167e-05,
      "loss": 1.2921,
      "step": 2210
    },
    {
      "epoch": 0.6357388316151202,
      "grad_norm": 3.4445693492889404,
      "learning_rate": 2.5541733547351525e-05,
      "loss": 2.1935,
      "step": 2220
    },
    {
      "epoch": 0.638602520045819,
      "grad_norm": 0.07333527505397797,
      "learning_rate": 2.5341091492776886e-05,
      "loss": 2.7186,
      "step": 2230
    },
    {
      "epoch": 0.6414662084765178,
      "grad_norm": 190.0563507080078,
      "learning_rate": 2.5140449438202248e-05,
      "loss": 4.1856,
      "step": 2240
    },
    {
      "epoch": 0.6443298969072165,
      "grad_norm": 22.50431251525879,
      "learning_rate": 2.493980738362761e-05,
      "loss": 2.2714,
      "step": 2250
    },
    {
      "epoch": 0.6471935853379153,
      "grad_norm": 0.9864017367362976,
      "learning_rate": 2.473916532905297e-05,
      "loss": 1.3202,
      "step": 2260
    },
    {
      "epoch": 0.650057273768614,
      "grad_norm": 0.7595638632774353,
      "learning_rate": 2.453852327447833e-05,
      "loss": 2.0986,
      "step": 2270
    },
    {
      "epoch": 0.6529209621993127,
      "grad_norm": 0.5519667267799377,
      "learning_rate": 2.4337881219903693e-05,
      "loss": 1.3686,
      "step": 2280
    },
    {
      "epoch": 0.6557846506300115,
      "grad_norm": 9.53549861907959,
      "learning_rate": 2.4137239165329054e-05,
      "loss": 0.115,
      "step": 2290
    },
    {
      "epoch": 0.6586483390607102,
      "grad_norm": 0.12279772758483887,
      "learning_rate": 2.3936597110754415e-05,
      "loss": 0.118,
      "step": 2300
    },
    {
      "epoch": 0.6615120274914089,
      "grad_norm": 185.24070739746094,
      "learning_rate": 2.3735955056179777e-05,
      "loss": 1.7764,
      "step": 2310
    },
    {
      "epoch": 0.6643757159221076,
      "grad_norm": 0.06734446436166763,
      "learning_rate": 2.3535313001605138e-05,
      "loss": 0.6756,
      "step": 2320
    },
    {
      "epoch": 0.6672394043528064,
      "grad_norm": 198.5089874267578,
      "learning_rate": 2.33346709470305e-05,
      "loss": 3.8573,
      "step": 2330
    },
    {
      "epoch": 0.6701030927835051,
      "grad_norm": 0.05416644364595413,
      "learning_rate": 2.3134028892455857e-05,
      "loss": 2.3958,
      "step": 2340
    },
    {
      "epoch": 0.672966781214204,
      "grad_norm": 1.160331130027771,
      "learning_rate": 2.2933386837881222e-05,
      "loss": 2.2634,
      "step": 2350
    },
    {
      "epoch": 0.6758304696449027,
      "grad_norm": 190.48403930664062,
      "learning_rate": 2.273274478330658e-05,
      "loss": 3.9075,
      "step": 2360
    },
    {
      "epoch": 0.6786941580756014,
      "grad_norm": 205.9942169189453,
      "learning_rate": 2.2532102728731944e-05,
      "loss": 1.2976,
      "step": 2370
    },
    {
      "epoch": 0.6815578465063001,
      "grad_norm": 11.202000617980957,
      "learning_rate": 2.2331460674157302e-05,
      "loss": 1.7356,
      "step": 2380
    },
    {
      "epoch": 0.6844215349369989,
      "grad_norm": 139.75750732421875,
      "learning_rate": 2.2130818619582667e-05,
      "loss": 1.8258,
      "step": 2390
    },
    {
      "epoch": 0.6872852233676976,
      "grad_norm": 0.5192713141441345,
      "learning_rate": 2.1930176565008025e-05,
      "loss": 0.2603,
      "step": 2400
    },
    {
      "epoch": 0.6901489117983963,
      "grad_norm": 1.4337959289550781,
      "learning_rate": 2.172953451043339e-05,
      "loss": 2.8508,
      "step": 2410
    },
    {
      "epoch": 0.693012600229095,
      "grad_norm": 184.8733367919922,
      "learning_rate": 2.1528892455858747e-05,
      "loss": 2.1649,
      "step": 2420
    },
    {
      "epoch": 0.6958762886597938,
      "grad_norm": 0.13650773465633392,
      "learning_rate": 2.1328250401284112e-05,
      "loss": 0.488,
      "step": 2430
    },
    {
      "epoch": 0.6987399770904925,
      "grad_norm": 0.32381653785705566,
      "learning_rate": 2.112760834670947e-05,
      "loss": 2.2096,
      "step": 2440
    },
    {
      "epoch": 0.7016036655211912,
      "grad_norm": 0.251362681388855,
      "learning_rate": 2.0926966292134835e-05,
      "loss": 1.9231,
      "step": 2450
    },
    {
      "epoch": 0.7044673539518901,
      "grad_norm": 17.53217887878418,
      "learning_rate": 2.0726324237560193e-05,
      "loss": 0.5211,
      "step": 2460
    },
    {
      "epoch": 0.7073310423825888,
      "grad_norm": 0.1673824042081833,
      "learning_rate": 2.0525682182985557e-05,
      "loss": 0.0069,
      "step": 2470
    },
    {
      "epoch": 0.7101947308132875,
      "grad_norm": 9.850105285644531,
      "learning_rate": 2.0325040128410915e-05,
      "loss": 2.2138,
      "step": 2480
    },
    {
      "epoch": 0.7130584192439863,
      "grad_norm": 3.626856565475464,
      "learning_rate": 2.0124398073836277e-05,
      "loss": 1.1407,
      "step": 2490
    },
    {
      "epoch": 0.715922107674685,
      "grad_norm": 1.352197289466858,
      "learning_rate": 1.9923756019261638e-05,
      "loss": 0.5093,
      "step": 2500
    },
    {
      "epoch": 0.7187857961053837,
      "grad_norm": 0.08264494687318802,
      "learning_rate": 1.9723113964687e-05,
      "loss": 0.8938,
      "step": 2510
    },
    {
      "epoch": 0.7216494845360825,
      "grad_norm": 0.016142448410391808,
      "learning_rate": 1.952247191011236e-05,
      "loss": 1.1708,
      "step": 2520
    },
    {
      "epoch": 0.7245131729667812,
      "grad_norm": 288.0754699707031,
      "learning_rate": 1.9321829855537722e-05,
      "loss": 1.2147,
      "step": 2530
    },
    {
      "epoch": 0.7273768613974799,
      "grad_norm": 75.53643798828125,
      "learning_rate": 1.9121187800963083e-05,
      "loss": 1.7291,
      "step": 2540
    },
    {
      "epoch": 0.7302405498281787,
      "grad_norm": 186.81640625,
      "learning_rate": 1.8920545746388444e-05,
      "loss": 3.5095,
      "step": 2550
    },
    {
      "epoch": 0.7331042382588774,
      "grad_norm": 45.5323600769043,
      "learning_rate": 1.8719903691813806e-05,
      "loss": 1.2765,
      "step": 2560
    },
    {
      "epoch": 0.7359679266895762,
      "grad_norm": 0.24278077483177185,
      "learning_rate": 1.8519261637239167e-05,
      "loss": 2.0289,
      "step": 2570
    },
    {
      "epoch": 0.738831615120275,
      "grad_norm": 0.004998129326850176,
      "learning_rate": 1.8318619582664528e-05,
      "loss": 0.2983,
      "step": 2580
    },
    {
      "epoch": 0.7416953035509737,
      "grad_norm": 1.1040536165237427,
      "learning_rate": 1.811797752808989e-05,
      "loss": 0.6839,
      "step": 2590
    },
    {
      "epoch": 0.7445589919816724,
      "grad_norm": 0.9869242310523987,
      "learning_rate": 1.791733547351525e-05,
      "loss": 2.6189,
      "step": 2600
    },
    {
      "epoch": 0.7474226804123711,
      "grad_norm": 0.6593422293663025,
      "learning_rate": 1.7716693418940612e-05,
      "loss": 1.6315,
      "step": 2610
    },
    {
      "epoch": 0.7502863688430699,
      "grad_norm": 187.03408813476562,
      "learning_rate": 1.751605136436597e-05,
      "loss": 1.6835,
      "step": 2620
    },
    {
      "epoch": 0.7531500572737686,
      "grad_norm": 0.2464132159948349,
      "learning_rate": 1.7315409309791335e-05,
      "loss": 2.4756,
      "step": 2630
    },
    {
      "epoch": 0.7560137457044673,
      "grad_norm": 2.51069974899292,
      "learning_rate": 1.7114767255216693e-05,
      "loss": 0.0071,
      "step": 2640
    },
    {
      "epoch": 0.7588774341351661,
      "grad_norm": 210.41665649414062,
      "learning_rate": 1.6914125200642057e-05,
      "loss": 2.2342,
      "step": 2650
    },
    {
      "epoch": 0.7617411225658648,
      "grad_norm": 203.19334411621094,
      "learning_rate": 1.6713483146067415e-05,
      "loss": 0.6859,
      "step": 2660
    },
    {
      "epoch": 0.7646048109965635,
      "grad_norm": 198.24392700195312,
      "learning_rate": 1.651284109149278e-05,
      "loss": 3.8172,
      "step": 2670
    },
    {
      "epoch": 0.7674684994272624,
      "grad_norm": 0.10993082821369171,
      "learning_rate": 1.6312199036918138e-05,
      "loss": 1.3954,
      "step": 2680
    },
    {
      "epoch": 0.7703321878579611,
      "grad_norm": 192.06640625,
      "learning_rate": 1.6111556982343502e-05,
      "loss": 0.942,
      "step": 2690
    },
    {
      "epoch": 0.7731958762886598,
      "grad_norm": 193.7117462158203,
      "learning_rate": 1.591091492776886e-05,
      "loss": 2.4246,
      "step": 2700
    },
    {
      "epoch": 0.7760595647193586,
      "grad_norm": 210.20738220214844,
      "learning_rate": 1.5710272873194225e-05,
      "loss": 0.4389,
      "step": 2710
    },
    {
      "epoch": 0.7789232531500573,
      "grad_norm": 208.7965545654297,
      "learning_rate": 1.5509630818619583e-05,
      "loss": 1.5316,
      "step": 2720
    },
    {
      "epoch": 0.781786941580756,
      "grad_norm": 0.04118012636899948,
      "learning_rate": 1.5308988764044944e-05,
      "loss": 0.8269,
      "step": 2730
    },
    {
      "epoch": 0.7846506300114547,
      "grad_norm": 0.09292957931756973,
      "learning_rate": 1.5108346709470305e-05,
      "loss": 4.0622,
      "step": 2740
    },
    {
      "epoch": 0.7875143184421535,
      "grad_norm": 0.26908472180366516,
      "learning_rate": 1.4907704654895665e-05,
      "loss": 1.8216,
      "step": 2750
    },
    {
      "epoch": 0.7903780068728522,
      "grad_norm": 192.50537109375,
      "learning_rate": 1.4707062600321028e-05,
      "loss": 3.3064,
      "step": 2760
    },
    {
      "epoch": 0.7932416953035509,
      "grad_norm": 231.1748504638672,
      "learning_rate": 1.4506420545746388e-05,
      "loss": 2.06,
      "step": 2770
    },
    {
      "epoch": 0.7961053837342497,
      "grad_norm": 1.5096673965454102,
      "learning_rate": 1.430577849117175e-05,
      "loss": 1.7246,
      "step": 2780
    },
    {
      "epoch": 0.7989690721649485,
      "grad_norm": 8.14208984375,
      "learning_rate": 1.410513643659711e-05,
      "loss": 1.5817,
      "step": 2790
    },
    {
      "epoch": 0.8018327605956472,
      "grad_norm": 184.73248291015625,
      "learning_rate": 1.3904494382022473e-05,
      "loss": 1.6142,
      "step": 2800
    },
    {
      "epoch": 0.804696449026346,
      "grad_norm": 210.48838806152344,
      "learning_rate": 1.3703852327447833e-05,
      "loss": 1.5877,
      "step": 2810
    },
    {
      "epoch": 0.8075601374570447,
      "grad_norm": 0.2491009682416916,
      "learning_rate": 1.3503210272873196e-05,
      "loss": 1.283,
      "step": 2820
    },
    {
      "epoch": 0.8104238258877434,
      "grad_norm": 183.5198974609375,
      "learning_rate": 1.3302568218298555e-05,
      "loss": 3.3555,
      "step": 2830
    },
    {
      "epoch": 0.8132875143184422,
      "grad_norm": 1.10878324508667,
      "learning_rate": 1.3101926163723918e-05,
      "loss": 1.4008,
      "step": 2840
    },
    {
      "epoch": 0.8161512027491409,
      "grad_norm": 33.876304626464844,
      "learning_rate": 1.2901284109149278e-05,
      "loss": 1.568,
      "step": 2850
    },
    {
      "epoch": 0.8190148911798396,
      "grad_norm": 198.70870971679688,
      "learning_rate": 1.2700642054574641e-05,
      "loss": 1.7055,
      "step": 2860
    },
    {
      "epoch": 0.8218785796105383,
      "grad_norm": 201.29930114746094,
      "learning_rate": 1.25e-05,
      "loss": 3.5748,
      "step": 2870
    },
    {
      "epoch": 0.8247422680412371,
      "grad_norm": 0.04147816076874733,
      "learning_rate": 1.2299357945425362e-05,
      "loss": 1.1382,
      "step": 2880
    },
    {
      "epoch": 0.8276059564719358,
      "grad_norm": 0.5273023843765259,
      "learning_rate": 1.2098715890850723e-05,
      "loss": 2.2686,
      "step": 2890
    },
    {
      "epoch": 0.8304696449026346,
      "grad_norm": 188.69277954101562,
      "learning_rate": 1.1898073836276084e-05,
      "loss": 0.6689,
      "step": 2900
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 204.08273315429688,
      "learning_rate": 1.1697431781701446e-05,
      "loss": 2.2173,
      "step": 2910
    },
    {
      "epoch": 0.8361970217640321,
      "grad_norm": 222.27114868164062,
      "learning_rate": 1.1496789727126807e-05,
      "loss": 3.0101,
      "step": 2920
    },
    {
      "epoch": 0.8390607101947308,
      "grad_norm": 145.09197998046875,
      "learning_rate": 1.1296147672552168e-05,
      "loss": 3.4006,
      "step": 2930
    },
    {
      "epoch": 0.8419243986254296,
      "grad_norm": 180.96847534179688,
      "learning_rate": 1.1095505617977528e-05,
      "loss": 2.2344,
      "step": 2940
    },
    {
      "epoch": 0.8447880870561283,
      "grad_norm": 0.6509403586387634,
      "learning_rate": 1.089486356340289e-05,
      "loss": 3.0802,
      "step": 2950
    },
    {
      "epoch": 0.847651775486827,
      "grad_norm": 1.5930860042572021,
      "learning_rate": 1.069422150882825e-05,
      "loss": 1.1985,
      "step": 2960
    },
    {
      "epoch": 0.8505154639175257,
      "grad_norm": 155.7156524658203,
      "learning_rate": 1.0493579454253612e-05,
      "loss": 0.9247,
      "step": 2970
    },
    {
      "epoch": 0.8533791523482245,
      "grad_norm": 1.2978111505508423,
      "learning_rate": 1.0292937399678973e-05,
      "loss": 1.0626,
      "step": 2980
    },
    {
      "epoch": 0.8562428407789232,
      "grad_norm": 1.2479515075683594,
      "learning_rate": 1.0092295345104334e-05,
      "loss": 2.1905,
      "step": 2990
    },
    {
      "epoch": 0.8591065292096219,
      "grad_norm": 13.488945960998535,
      "learning_rate": 9.891653290529696e-06,
      "loss": 1.5555,
      "step": 3000
    },
    {
      "epoch": 0.8591065292096219,
      "eval_loss": 1.7668182849884033,
      "eval_runtime": 12.233,
      "eval_samples_per_second": 38.829,
      "eval_steps_per_second": 4.905,
      "step": 3000
    },
    {
      "epoch": 0.8619702176403208,
      "grad_norm": 0.5121981501579285,
      "learning_rate": 9.691011235955057e-06,
      "loss": 2.0666,
      "step": 3010
    },
    {
      "epoch": 0.8648339060710195,
      "grad_norm": 0.5610802173614502,
      "learning_rate": 9.490369181380418e-06,
      "loss": 1.815,
      "step": 3020
    },
    {
      "epoch": 0.8676975945017182,
      "grad_norm": 0.46904438734054565,
      "learning_rate": 9.28972712680578e-06,
      "loss": 1.3336,
      "step": 3030
    },
    {
      "epoch": 0.870561282932417,
      "grad_norm": 186.39044189453125,
      "learning_rate": 9.089085072231141e-06,
      "loss": 0.7514,
      "step": 3040
    },
    {
      "epoch": 0.8734249713631157,
      "grad_norm": 2.75667142868042,
      "learning_rate": 8.888443017656502e-06,
      "loss": 2.9643,
      "step": 3050
    },
    {
      "epoch": 0.8762886597938144,
      "grad_norm": 0.2593824863433838,
      "learning_rate": 8.687800963081863e-06,
      "loss": 2.4008,
      "step": 3060
    },
    {
      "epoch": 0.8791523482245132,
      "grad_norm": 0.16063332557678223,
      "learning_rate": 8.487158908507223e-06,
      "loss": 1.7681,
      "step": 3070
    },
    {
      "epoch": 0.8820160366552119,
      "grad_norm": 25.97922706604004,
      "learning_rate": 8.286516853932584e-06,
      "loss": 1.6673,
      "step": 3080
    },
    {
      "epoch": 0.8848797250859106,
      "grad_norm": 0.5919740200042725,
      "learning_rate": 8.085874799357946e-06,
      "loss": 2.7587,
      "step": 3090
    },
    {
      "epoch": 0.8877434135166093,
      "grad_norm": 200.44715881347656,
      "learning_rate": 7.885232744783307e-06,
      "loss": 2.5884,
      "step": 3100
    },
    {
      "epoch": 0.8906071019473081,
      "grad_norm": 0.14439362287521362,
      "learning_rate": 7.684590690208668e-06,
      "loss": 0.1558,
      "step": 3110
    },
    {
      "epoch": 0.8934707903780069,
      "grad_norm": 31.28517723083496,
      "learning_rate": 7.483948635634029e-06,
      "loss": 3.1662,
      "step": 3120
    },
    {
      "epoch": 0.8963344788087056,
      "grad_norm": 217.54791259765625,
      "learning_rate": 7.28330658105939e-06,
      "loss": 2.3658,
      "step": 3130
    },
    {
      "epoch": 0.8991981672394044,
      "grad_norm": 0.9032984972000122,
      "learning_rate": 7.082664526484751e-06,
      "loss": 1.6928,
      "step": 3140
    },
    {
      "epoch": 0.9020618556701031,
      "grad_norm": 0.2150217592716217,
      "learning_rate": 6.8820224719101126e-06,
      "loss": 1.5456,
      "step": 3150
    },
    {
      "epoch": 0.9049255441008018,
      "grad_norm": 0.6457381248474121,
      "learning_rate": 6.681380417335474e-06,
      "loss": 1.9181,
      "step": 3160
    },
    {
      "epoch": 0.9077892325315006,
      "grad_norm": 0.6596923470497131,
      "learning_rate": 6.480738362760835e-06,
      "loss": 3.2961,
      "step": 3170
    },
    {
      "epoch": 0.9106529209621993,
      "grad_norm": 21.830352783203125,
      "learning_rate": 6.2800963081861964e-06,
      "loss": 1.3155,
      "step": 3180
    },
    {
      "epoch": 0.913516609392898,
      "grad_norm": 0.455312043428421,
      "learning_rate": 6.079454253611557e-06,
      "loss": 0.0125,
      "step": 3190
    },
    {
      "epoch": 0.9163802978235968,
      "grad_norm": 195.98110961914062,
      "learning_rate": 5.878812199036918e-06,
      "loss": 1.5499,
      "step": 3200
    },
    {
      "epoch": 0.9192439862542955,
      "grad_norm": 203.28419494628906,
      "learning_rate": 5.6781701444622795e-06,
      "loss": 0.3592,
      "step": 3210
    },
    {
      "epoch": 0.9221076746849943,
      "grad_norm": 183.48854064941406,
      "learning_rate": 5.477528089887641e-06,
      "loss": 0.2364,
      "step": 3220
    },
    {
      "epoch": 0.9249713631156931,
      "grad_norm": 178.3472137451172,
      "learning_rate": 5.276886035313002e-06,
      "loss": 3.0639,
      "step": 3230
    },
    {
      "epoch": 0.9278350515463918,
      "grad_norm": 0.03631196543574333,
      "learning_rate": 5.0762439807383625e-06,
      "loss": 1.0132,
      "step": 3240
    },
    {
      "epoch": 0.9306987399770905,
      "grad_norm": 224.552734375,
      "learning_rate": 4.875601926163724e-06,
      "loss": 2.5064,
      "step": 3250
    },
    {
      "epoch": 0.9335624284077892,
      "grad_norm": 0.23594437539577484,
      "learning_rate": 4.674959871589085e-06,
      "loss": 1.2605,
      "step": 3260
    },
    {
      "epoch": 0.936426116838488,
      "grad_norm": 0.16237188875675201,
      "learning_rate": 4.474317817014446e-06,
      "loss": 2.0363,
      "step": 3270
    },
    {
      "epoch": 0.9392898052691867,
      "grad_norm": 1.0884993076324463,
      "learning_rate": 4.273675762439808e-06,
      "loss": 1.7293,
      "step": 3280
    },
    {
      "epoch": 0.9421534936998854,
      "grad_norm": 227.35894775390625,
      "learning_rate": 4.073033707865169e-06,
      "loss": 2.0915,
      "step": 3290
    },
    {
      "epoch": 0.9450171821305842,
      "grad_norm": 193.9648895263672,
      "learning_rate": 3.87239165329053e-06,
      "loss": 2.2352,
      "step": 3300
    },
    {
      "epoch": 0.9478808705612829,
      "grad_norm": 187.97328186035156,
      "learning_rate": 3.6717495987158907e-06,
      "loss": 1.5142,
      "step": 3310
    },
    {
      "epoch": 0.9507445589919816,
      "grad_norm": 0.5644872784614563,
      "learning_rate": 3.471107544141252e-06,
      "loss": 0.9218,
      "step": 3320
    },
    {
      "epoch": 0.9536082474226805,
      "grad_norm": 0.09489203989505768,
      "learning_rate": 3.2704654895666133e-06,
      "loss": 0.8858,
      "step": 3330
    },
    {
      "epoch": 0.9564719358533792,
      "grad_norm": 203.68263244628906,
      "learning_rate": 3.0698234349919746e-06,
      "loss": 1.1643,
      "step": 3340
    },
    {
      "epoch": 0.9593356242840779,
      "grad_norm": 185.8425750732422,
      "learning_rate": 2.869181380417336e-06,
      "loss": 1.975,
      "step": 3350
    },
    {
      "epoch": 0.9621993127147767,
      "grad_norm": 0.016092823818325996,
      "learning_rate": 2.6685393258426968e-06,
      "loss": 2.3796,
      "step": 3360
    },
    {
      "epoch": 0.9650630011454754,
      "grad_norm": 1.007483959197998,
      "learning_rate": 2.467897271268058e-06,
      "loss": 1.3154,
      "step": 3370
    },
    {
      "epoch": 0.9679266895761741,
      "grad_norm": 190.979248046875,
      "learning_rate": 2.2672552166934193e-06,
      "loss": 1.9252,
      "step": 3380
    },
    {
      "epoch": 0.9707903780068728,
      "grad_norm": 21.556215286254883,
      "learning_rate": 2.06661316211878e-06,
      "loss": 1.9881,
      "step": 3390
    },
    {
      "epoch": 0.9736540664375716,
      "grad_norm": 108.6590347290039,
      "learning_rate": 1.8659711075441413e-06,
      "loss": 2.4763,
      "step": 3400
    },
    {
      "epoch": 0.9765177548682703,
      "grad_norm": 214.42063903808594,
      "learning_rate": 1.6653290529695026e-06,
      "loss": 3.8011,
      "step": 3410
    },
    {
      "epoch": 0.979381443298969,
      "grad_norm": 152.92832946777344,
      "learning_rate": 1.4646869983948637e-06,
      "loss": 1.107,
      "step": 3420
    },
    {
      "epoch": 0.9822451317296678,
      "grad_norm": 242.87014770507812,
      "learning_rate": 1.2640449438202247e-06,
      "loss": 4.1087,
      "step": 3430
    },
    {
      "epoch": 0.9851088201603666,
      "grad_norm": 0.001830819877795875,
      "learning_rate": 1.063402889245586e-06,
      "loss": 1.5261,
      "step": 3440
    },
    {
      "epoch": 0.9879725085910653,
      "grad_norm": 1.523565649986267,
      "learning_rate": 8.627608346709471e-07,
      "loss": 0.9479,
      "step": 3450
    },
    {
      "epoch": 0.9908361970217641,
      "grad_norm": 0.28587883710861206,
      "learning_rate": 6.621187800963082e-07,
      "loss": 1.514,
      "step": 3460
    },
    {
      "epoch": 0.9936998854524628,
      "grad_norm": 190.7532196044922,
      "learning_rate": 4.614767255216694e-07,
      "loss": 1.1316,
      "step": 3470
    },
    {
      "epoch": 0.9965635738831615,
      "grad_norm": 0.7220925092697144,
      "learning_rate": 2.608346709470305e-07,
      "loss": 3.7168,
      "step": 3480
    },
    {
      "epoch": 0.9994272623138603,
      "grad_norm": 132.6321258544922,
      "learning_rate": 6.019261637239165e-08,
      "loss": 1.8825,
      "step": 3490
    }
  ],
  "logging_steps": 10,
  "max_steps": 3492,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 1000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.19267701121024e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
