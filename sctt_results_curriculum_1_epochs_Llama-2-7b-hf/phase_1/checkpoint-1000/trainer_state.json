{
  "best_global_step": 1000,
  "best_metric": 1.1588571071624756,
  "best_model_checkpoint": "./sctt_results_curriculum_1_epochs_Llama-2-7b-hf/phase_1\\checkpoint-1000",
  "epoch": 0.286368843069874,
  "eval_steps": 1000,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0028636884306987398,
      "grad_norm": 79.57469940185547,
      "learning_rate": 4.5e-07,
      "loss": 0.9246,
      "step": 10
    },
    {
      "epoch": 0.0057273768613974796,
      "grad_norm": 127.5727767944336,
      "learning_rate": 9.5e-07,
      "loss": 0.8738,
      "step": 20
    },
    {
      "epoch": 0.00859106529209622,
      "grad_norm": 87.35374450683594,
      "learning_rate": 1.45e-06,
      "loss": 0.7182,
      "step": 30
    },
    {
      "epoch": 0.011454753722794959,
      "grad_norm": 155.85159301757812,
      "learning_rate": 1.95e-06,
      "loss": 1.7947,
      "step": 40
    },
    {
      "epoch": 0.0143184421534937,
      "grad_norm": 96.61840057373047,
      "learning_rate": 2.4500000000000003e-06,
      "loss": 1.699,
      "step": 50
    },
    {
      "epoch": 0.01718213058419244,
      "grad_norm": 51.01338195800781,
      "learning_rate": 2.95e-06,
      "loss": 1.823,
      "step": 60
    },
    {
      "epoch": 0.02004581901489118,
      "grad_norm": 86.18168640136719,
      "learning_rate": 3.4500000000000004e-06,
      "loss": 1.059,
      "step": 70
    },
    {
      "epoch": 0.022909507445589918,
      "grad_norm": 29.880569458007812,
      "learning_rate": 3.95e-06,
      "loss": 1.0621,
      "step": 80
    },
    {
      "epoch": 0.02577319587628866,
      "grad_norm": 142.8107452392578,
      "learning_rate": 4.45e-06,
      "loss": 1.2547,
      "step": 90
    },
    {
      "epoch": 0.0286368843069874,
      "grad_norm": 150.6685028076172,
      "learning_rate": 4.950000000000001e-06,
      "loss": 0.8234,
      "step": 100
    },
    {
      "epoch": 0.03150057273768614,
      "grad_norm": 79.3604736328125,
      "learning_rate": 5.45e-06,
      "loss": 1.335,
      "step": 110
    },
    {
      "epoch": 0.03436426116838488,
      "grad_norm": 57.21981430053711,
      "learning_rate": 5.95e-06,
      "loss": 0.9573,
      "step": 120
    },
    {
      "epoch": 0.03722794959908362,
      "grad_norm": 14.749129295349121,
      "learning_rate": 6.45e-06,
      "loss": 0.6009,
      "step": 130
    },
    {
      "epoch": 0.04009163802978236,
      "grad_norm": 144.7097930908203,
      "learning_rate": 6.950000000000001e-06,
      "loss": 2.0102,
      "step": 140
    },
    {
      "epoch": 0.0429553264604811,
      "grad_norm": 11.893170356750488,
      "learning_rate": 7.45e-06,
      "loss": 0.7426,
      "step": 150
    },
    {
      "epoch": 0.045819014891179836,
      "grad_norm": 26.94629669189453,
      "learning_rate": 7.95e-06,
      "loss": 0.5908,
      "step": 160
    },
    {
      "epoch": 0.04868270332187858,
      "grad_norm": 52.624820709228516,
      "learning_rate": 8.45e-06,
      "loss": 0.6422,
      "step": 170
    },
    {
      "epoch": 0.05154639175257732,
      "grad_norm": 33.033180236816406,
      "learning_rate": 8.95e-06,
      "loss": 1.1016,
      "step": 180
    },
    {
      "epoch": 0.05441008018327606,
      "grad_norm": 144.33352661132812,
      "learning_rate": 9.450000000000001e-06,
      "loss": 1.3828,
      "step": 190
    },
    {
      "epoch": 0.0572737686139748,
      "grad_norm": 157.7412109375,
      "learning_rate": 9.950000000000001e-06,
      "loss": 1.0101,
      "step": 200
    },
    {
      "epoch": 0.06013745704467354,
      "grad_norm": 177.59027099609375,
      "learning_rate": 1.045e-05,
      "loss": 2.3439,
      "step": 210
    },
    {
      "epoch": 0.06300114547537228,
      "grad_norm": 65.52720642089844,
      "learning_rate": 1.095e-05,
      "loss": 0.8731,
      "step": 220
    },
    {
      "epoch": 0.06586483390607102,
      "grad_norm": 169.60415649414062,
      "learning_rate": 1.145e-05,
      "loss": 1.5438,
      "step": 230
    },
    {
      "epoch": 0.06872852233676977,
      "grad_norm": 75.37397766113281,
      "learning_rate": 1.195e-05,
      "loss": 1.735,
      "step": 240
    },
    {
      "epoch": 0.0715922107674685,
      "grad_norm": 153.44161987304688,
      "learning_rate": 1.2450000000000001e-05,
      "loss": 2.4232,
      "step": 250
    },
    {
      "epoch": 0.07445589919816724,
      "grad_norm": 153.08489990234375,
      "learning_rate": 1.2950000000000001e-05,
      "loss": 1.427,
      "step": 260
    },
    {
      "epoch": 0.07731958762886598,
      "grad_norm": 23.26593780517578,
      "learning_rate": 1.3450000000000002e-05,
      "loss": 1.6374,
      "step": 270
    },
    {
      "epoch": 0.08018327605956473,
      "grad_norm": 172.59397888183594,
      "learning_rate": 1.3950000000000002e-05,
      "loss": 1.6996,
      "step": 280
    },
    {
      "epoch": 0.08304696449026346,
      "grad_norm": 39.52104568481445,
      "learning_rate": 1.4449999999999999e-05,
      "loss": 2.2068,
      "step": 290
    },
    {
      "epoch": 0.0859106529209622,
      "grad_norm": 19.315290451049805,
      "learning_rate": 1.4950000000000001e-05,
      "loss": 1.2206,
      "step": 300
    },
    {
      "epoch": 0.08877434135166094,
      "grad_norm": 137.10704040527344,
      "learning_rate": 1.545e-05,
      "loss": 0.8561,
      "step": 310
    },
    {
      "epoch": 0.09163802978235967,
      "grad_norm": 3.5110836029052734,
      "learning_rate": 1.595e-05,
      "loss": 2.4104,
      "step": 320
    },
    {
      "epoch": 0.09450171821305842,
      "grad_norm": 129.2257843017578,
      "learning_rate": 1.645e-05,
      "loss": 1.7295,
      "step": 330
    },
    {
      "epoch": 0.09736540664375716,
      "grad_norm": 28.052364349365234,
      "learning_rate": 1.6950000000000002e-05,
      "loss": 0.523,
      "step": 340
    },
    {
      "epoch": 0.1002290950744559,
      "grad_norm": 163.10592651367188,
      "learning_rate": 1.745e-05,
      "loss": 1.0118,
      "step": 350
    },
    {
      "epoch": 0.10309278350515463,
      "grad_norm": 124.40232849121094,
      "learning_rate": 1.795e-05,
      "loss": 1.9154,
      "step": 360
    },
    {
      "epoch": 0.10595647193585338,
      "grad_norm": 98.40947723388672,
      "learning_rate": 1.845e-05,
      "loss": 0.8547,
      "step": 370
    },
    {
      "epoch": 0.10882016036655212,
      "grad_norm": 28.401222229003906,
      "learning_rate": 1.895e-05,
      "loss": 1.0301,
      "step": 380
    },
    {
      "epoch": 0.11168384879725086,
      "grad_norm": 53.26777267456055,
      "learning_rate": 1.9450000000000002e-05,
      "loss": 1.9111,
      "step": 390
    },
    {
      "epoch": 0.1145475372279496,
      "grad_norm": 148.125,
      "learning_rate": 1.995e-05,
      "loss": 1.624,
      "step": 400
    },
    {
      "epoch": 0.11741122565864834,
      "grad_norm": 174.4932403564453,
      "learning_rate": 2.045e-05,
      "loss": 1.1434,
      "step": 410
    },
    {
      "epoch": 0.12027491408934708,
      "grad_norm": 178.27789306640625,
      "learning_rate": 2.095e-05,
      "loss": 2.0094,
      "step": 420
    },
    {
      "epoch": 0.12313860252004583,
      "grad_norm": 153.8032989501953,
      "learning_rate": 2.145e-05,
      "loss": 2.6538,
      "step": 430
    },
    {
      "epoch": 0.12600229095074456,
      "grad_norm": 162.99942016601562,
      "learning_rate": 2.195e-05,
      "loss": 1.3631,
      "step": 440
    },
    {
      "epoch": 0.12886597938144329,
      "grad_norm": 173.99794006347656,
      "learning_rate": 2.245e-05,
      "loss": 1.8303,
      "step": 450
    },
    {
      "epoch": 0.13172966781214204,
      "grad_norm": 2.1185061931610107,
      "learning_rate": 2.2950000000000002e-05,
      "loss": 1.5457,
      "step": 460
    },
    {
      "epoch": 0.13459335624284077,
      "grad_norm": 12.151549339294434,
      "learning_rate": 2.345e-05,
      "loss": 1.7921,
      "step": 470
    },
    {
      "epoch": 0.13745704467353953,
      "grad_norm": 123.99901580810547,
      "learning_rate": 2.395e-05,
      "loss": 1.7476,
      "step": 480
    },
    {
      "epoch": 0.14032073310423826,
      "grad_norm": 79.57270812988281,
      "learning_rate": 2.445e-05,
      "loss": 0.7064,
      "step": 490
    },
    {
      "epoch": 0.143184421534937,
      "grad_norm": 7.035595893859863,
      "learning_rate": 2.495e-05,
      "loss": 1.1987,
      "step": 500
    },
    {
      "epoch": 0.14604810996563575,
      "grad_norm": 90.12335205078125,
      "learning_rate": 2.5450000000000002e-05,
      "loss": 1.7331,
      "step": 510
    },
    {
      "epoch": 0.14891179839633448,
      "grad_norm": 10.40755558013916,
      "learning_rate": 2.595e-05,
      "loss": 1.3956,
      "step": 520
    },
    {
      "epoch": 0.1517754868270332,
      "grad_norm": 11.554871559143066,
      "learning_rate": 2.6450000000000003e-05,
      "loss": 1.6892,
      "step": 530
    },
    {
      "epoch": 0.15463917525773196,
      "grad_norm": 105.87930297851562,
      "learning_rate": 2.6950000000000005e-05,
      "loss": 1.1927,
      "step": 540
    },
    {
      "epoch": 0.1575028636884307,
      "grad_norm": 174.60321044921875,
      "learning_rate": 2.7450000000000003e-05,
      "loss": 2.2742,
      "step": 550
    },
    {
      "epoch": 0.16036655211912945,
      "grad_norm": 154.6477813720703,
      "learning_rate": 2.7950000000000005e-05,
      "loss": 1.5865,
      "step": 560
    },
    {
      "epoch": 0.16323024054982818,
      "grad_norm": 3.523946523666382,
      "learning_rate": 2.845e-05,
      "loss": 1.387,
      "step": 570
    },
    {
      "epoch": 0.1660939289805269,
      "grad_norm": 8.066291809082031,
      "learning_rate": 2.895e-05,
      "loss": 1.854,
      "step": 580
    },
    {
      "epoch": 0.16895761741122567,
      "grad_norm": 0.004578485619276762,
      "learning_rate": 2.945e-05,
      "loss": 0.7791,
      "step": 590
    },
    {
      "epoch": 0.1718213058419244,
      "grad_norm": 193.717041015625,
      "learning_rate": 2.995e-05,
      "loss": 2.8751,
      "step": 600
    },
    {
      "epoch": 0.17468499427262313,
      "grad_norm": 162.1082305908203,
      "learning_rate": 3.045e-05,
      "loss": 4.4869,
      "step": 610
    },
    {
      "epoch": 0.1775486827033219,
      "grad_norm": 10.160661697387695,
      "learning_rate": 3.095e-05,
      "loss": 1.9618,
      "step": 620
    },
    {
      "epoch": 0.18041237113402062,
      "grad_norm": 180.88133239746094,
      "learning_rate": 3.145e-05,
      "loss": 1.1641,
      "step": 630
    },
    {
      "epoch": 0.18327605956471935,
      "grad_norm": 4.278468132019043,
      "learning_rate": 3.1950000000000004e-05,
      "loss": 0.8125,
      "step": 640
    },
    {
      "epoch": 0.1861397479954181,
      "grad_norm": 111.17979431152344,
      "learning_rate": 3.245e-05,
      "loss": 0.9773,
      "step": 650
    },
    {
      "epoch": 0.18900343642611683,
      "grad_norm": 72.0273208618164,
      "learning_rate": 3.295e-05,
      "loss": 1.5189,
      "step": 660
    },
    {
      "epoch": 0.1918671248568156,
      "grad_norm": 22.57496452331543,
      "learning_rate": 3.345000000000001e-05,
      "loss": 0.4369,
      "step": 670
    },
    {
      "epoch": 0.19473081328751432,
      "grad_norm": 145.76510620117188,
      "learning_rate": 3.3950000000000005e-05,
      "loss": 1.9305,
      "step": 680
    },
    {
      "epoch": 0.19759450171821305,
      "grad_norm": 78.20345306396484,
      "learning_rate": 3.445e-05,
      "loss": 0.5265,
      "step": 690
    },
    {
      "epoch": 0.2004581901489118,
      "grad_norm": 177.4789276123047,
      "learning_rate": 3.495e-05,
      "loss": 1.9609,
      "step": 700
    },
    {
      "epoch": 0.20332187857961054,
      "grad_norm": 175.78622436523438,
      "learning_rate": 3.545e-05,
      "loss": 1.0614,
      "step": 710
    },
    {
      "epoch": 0.20618556701030927,
      "grad_norm": 181.46957397460938,
      "learning_rate": 3.595e-05,
      "loss": 0.9133,
      "step": 720
    },
    {
      "epoch": 0.20904925544100803,
      "grad_norm": 166.73020935058594,
      "learning_rate": 3.645e-05,
      "loss": 3.0033,
      "step": 730
    },
    {
      "epoch": 0.21191294387170675,
      "grad_norm": 177.95834350585938,
      "learning_rate": 3.6950000000000004e-05,
      "loss": 1.3657,
      "step": 740
    },
    {
      "epoch": 0.21477663230240548,
      "grad_norm": 178.87045288085938,
      "learning_rate": 3.745e-05,
      "loss": 2.3968,
      "step": 750
    },
    {
      "epoch": 0.21764032073310424,
      "grad_norm": 3.0540931224823,
      "learning_rate": 3.795e-05,
      "loss": 1.143,
      "step": 760
    },
    {
      "epoch": 0.22050400916380297,
      "grad_norm": 178.50057983398438,
      "learning_rate": 3.845e-05,
      "loss": 2.4636,
      "step": 770
    },
    {
      "epoch": 0.22336769759450173,
      "grad_norm": 1.2395052909851074,
      "learning_rate": 3.8950000000000005e-05,
      "loss": 2.9573,
      "step": 780
    },
    {
      "epoch": 0.22623138602520046,
      "grad_norm": 4.233154296875,
      "learning_rate": 3.9450000000000003e-05,
      "loss": 1.8664,
      "step": 790
    },
    {
      "epoch": 0.2290950744558992,
      "grad_norm": 6.921467304229736,
      "learning_rate": 3.995e-05,
      "loss": 1.5884,
      "step": 800
    },
    {
      "epoch": 0.23195876288659795,
      "grad_norm": 170.27525329589844,
      "learning_rate": 4.045000000000001e-05,
      "loss": 2.5943,
      "step": 810
    },
    {
      "epoch": 0.23482245131729668,
      "grad_norm": 118.09656524658203,
      "learning_rate": 4.095e-05,
      "loss": 1.3289,
      "step": 820
    },
    {
      "epoch": 0.2376861397479954,
      "grad_norm": 162.45057678222656,
      "learning_rate": 4.145e-05,
      "loss": 1.3327,
      "step": 830
    },
    {
      "epoch": 0.24054982817869416,
      "grad_norm": 87.85939025878906,
      "learning_rate": 4.195e-05,
      "loss": 1.495,
      "step": 840
    },
    {
      "epoch": 0.2434135166093929,
      "grad_norm": 180.3460235595703,
      "learning_rate": 4.245e-05,
      "loss": 1.1139,
      "step": 850
    },
    {
      "epoch": 0.24627720504009165,
      "grad_norm": 193.12057495117188,
      "learning_rate": 4.295e-05,
      "loss": 2.4603,
      "step": 860
    },
    {
      "epoch": 0.24914089347079038,
      "grad_norm": 139.45721435546875,
      "learning_rate": 4.345e-05,
      "loss": 1.5064,
      "step": 870
    },
    {
      "epoch": 0.2520045819014891,
      "grad_norm": 134.2423553466797,
      "learning_rate": 4.3950000000000004e-05,
      "loss": 1.8895,
      "step": 880
    },
    {
      "epoch": 0.25486827033218784,
      "grad_norm": 176.252197265625,
      "learning_rate": 4.445e-05,
      "loss": 2.7051,
      "step": 890
    },
    {
      "epoch": 0.25773195876288657,
      "grad_norm": 48.58606719970703,
      "learning_rate": 4.495e-05,
      "loss": 1.0121,
      "step": 900
    },
    {
      "epoch": 0.26059564719358536,
      "grad_norm": 7.177842140197754,
      "learning_rate": 4.545000000000001e-05,
      "loss": 0.9837,
      "step": 910
    },
    {
      "epoch": 0.2634593356242841,
      "grad_norm": 63.78435516357422,
      "learning_rate": 4.5950000000000006e-05,
      "loss": 1.2463,
      "step": 920
    },
    {
      "epoch": 0.2663230240549828,
      "grad_norm": 64.1390609741211,
      "learning_rate": 4.6450000000000004e-05,
      "loss": 0.861,
      "step": 930
    },
    {
      "epoch": 0.26918671248568155,
      "grad_norm": 69.67255401611328,
      "learning_rate": 4.695e-05,
      "loss": 1.2284,
      "step": 940
    },
    {
      "epoch": 0.2720504009163803,
      "grad_norm": 15.257335662841797,
      "learning_rate": 4.745e-05,
      "loss": 1.9696,
      "step": 950
    },
    {
      "epoch": 0.27491408934707906,
      "grad_norm": 1.3151649236679077,
      "learning_rate": 4.795e-05,
      "loss": 1.5772,
      "step": 960
    },
    {
      "epoch": 0.2777777777777778,
      "grad_norm": 0.1367829442024231,
      "learning_rate": 4.845e-05,
      "loss": 3.9821,
      "step": 970
    },
    {
      "epoch": 0.2806414662084765,
      "grad_norm": 0.7424548268318176,
      "learning_rate": 4.8950000000000004e-05,
      "loss": 3.8847,
      "step": 980
    },
    {
      "epoch": 0.28350515463917525,
      "grad_norm": 92.8092041015625,
      "learning_rate": 4.945e-05,
      "loss": 1.8531,
      "step": 990
    },
    {
      "epoch": 0.286368843069874,
      "grad_norm": 50.961883544921875,
      "learning_rate": 4.995e-05,
      "loss": 1.5218,
      "step": 1000
    },
    {
      "epoch": 0.286368843069874,
      "eval_loss": 1.1588571071624756,
      "eval_runtime": 12.3277,
      "eval_samples_per_second": 38.531,
      "eval_steps_per_second": 4.867,
      "step": 1000
    }
  ],
  "logging_steps": 10,
  "max_steps": 3492,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 1000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 6269944559763456.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
